{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c538479-3738-4862-8a2f-0fdc67469d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import argparse\n",
    "import logging\n",
    "from typing import List\n",
    "import torch\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "from sentence_transformers import CrossEncoder\n",
    "import os\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4731015e-96e2-44e1-b673-7dc80b623792",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT =Path(r\"C:\\Users\\Archit\\Documents\\ML Projects\\D-RAGon_System\")\n",
    "DATA_DIR = ROOT/'Data'\n",
    "CHROMA_DIR = ROOT/'Chroma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ada8f79b-8aec-4231-8c17-946f1caad614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Archit\\Documents\\ML Projects\\D-RAGon_System\n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\D-RAGon_System\\Data\n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\D-RAGon_System\\Chroma\n"
     ]
    }
   ],
   "source": [
    "print(ROOT)\n",
    "print(DATA_DIR)\n",
    "print(CHROMA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea5829f7-655e-45d9-9133-6111f1268c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs():\n",
    "    doc_loader = PyPDFDirectoryLoader(DATA_DIR)\n",
    "    return doc_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "326a933f-d557-42c4-a587-4981a4d814f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pages(docs, min_chars = 200):\n",
    "    cleaned = []\n",
    "    blacklist = [\n",
    "        \"all rights reserved\",\n",
    "        \"copyright\",\n",
    "        \"isbn\",\n",
    "        \"table of contents\"\n",
    "    ]\n",
    "    for d in docs:\n",
    "        text = d.page_content.lower()\n",
    "\n",
    "        if len(text)<min_chars: #removes short pages\n",
    "            continue\n",
    "        if any(b in text for b in blacklist): # removes boilerplate pages\n",
    "            continue\n",
    "        cleaned.append(d)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfe2bdd8-c393-4aa1-8b69-77ea18986de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_docs(docs: List):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,)\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    chunks = [c for c in chunks if len(c.page_content) > 200]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b109c204-42e3-4f8d-92db-3665a9797157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='CONTENTS\n",
      "INTRODUCTION\n",
      "1. I SHOULD HAVE BEEN A STATISTIC\n",
      "2. TRUTH HURTS\n",
      "3. THE IMPOSSIBLE TASK\n",
      "4. TAKING SOULS\n",
      "5. ARMORED MIND\n",
      "6. IT’S NOT ABOUT A TROPHY\n",
      "7. THE MOST POWERFUL WEAPON\n",
      "8. TALENT NOT REQUIRED\n",
      "9. UNCOMMON AMONGST UNCOMMON\n",
      "10. THE EMPOWERMENT OF FAILURE\n",
      "11. WHAT IF?\n",
      "ACKNOWLEDGMENTS\n",
      "ABOUT THE AUTHOR' metadata={'producer': 'calibre (2.85.1) [https://calibre-ebook.com]', 'creator': 'calibre (2.85.1) [https://calibre-ebook.com]', 'creationdate': '2020-06-25T21:00:51+00:00', 'author': 'David Goggins', 'moddate': '2020-06-25T21:01:00+00:00', 'title': \"Can't Hurt Me: Master Your Mind and Defy the Odds\", 'source': 'C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Can_t-Hurt-Me-David-Goggins.pdf', 'total_pages': 303, 'page': 3, 'page_label': '4'}\n"
     ]
    }
   ],
   "source": [
    "docs = load_docs()\n",
    "docs = filter_pages(docs)\n",
    "\n",
    "chunks = split_docs(docs)\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf57a40e-1244-403a-ae92-417246ef9c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299\n",
      "959\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a6a1a26-5fc7-45f3-a7d3-744f08e866f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_chunk_ids(chunks: List):\n",
    "    # This will create IDs like \"data/monopoly.pdf:6:2\"\n",
    "    # Page Source : Page Number : Chunk Index\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91cbfae6-b0ad-4ccb-84f9-5a6a1ed74e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_function(device: str = None):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    embeddings = HuggingFaceBgeEmbeddings(\n",
    "        model_name = 'BAAI/bge-large-en-v1.5',\n",
    "        model_kwargs={'device':'cuda'},\n",
    "        encode_kwargs={'normalize_embeddings':True},\n",
    "    )\n",
    "    return embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5180325-09a0-4b28-87cf-34eb33a809ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_chroma(chunks: List):\n",
    "    # load DB\n",
    "    db = Chroma(persist_directory=CHROMA_DIR,\n",
    "                embedding_function = get_embeddings_function())\n",
    "\n",
    "    #Calc Page IDs \n",
    "    chunks_with_ids = calc_chunk_ids(chunks)\n",
    "\n",
    "    # Add or update the documents\n",
    "    existing_items = db.get(include=[])\n",
    "    existing_ids = set(existing_items['ids'])\n",
    "    print(f\"Number of existing dicuments in DB: {len(existing_ids)}\")\n",
    "\n",
    "    # Only add docs that don't exist in the DB\n",
    "    new_chunks = []\n",
    "    for chunk in chunks_with_ids:\n",
    "        if chunk.metadata['id'] not in existing_ids:\n",
    "            new_chunks.append(chunk)\n",
    "    \n",
    "    if len(new_chunks):\n",
    "        print(f'Adding new documents: {len(new_chunks)}')\n",
    "        new_chunk_ids = [chunk.metadata['id'] for chunk in new_chunks]\n",
    "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "        #db.persist()\n",
    "    else:\n",
    "        print(\"No New Documents to add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c6ef9f1-9135-487b-8ed4-e2f88e8527de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0412e621420c4c099e70d6801dd1d80c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67c8535e9814114ae4613e85b9ee099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Initialize Embeddings\n",
    "emb_fxn = get_embeddings_function()\n",
    "\n",
    "# Initialize Vector Store\n",
    "db = Chroma(\n",
    "    persist_directory=CHROMA_DIR,\n",
    "    embedding_function=emb_fxn\n",
    ")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OllamaLLM(model='llama3.1')\n",
    "\n",
    "# Cross encoder\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9f9b485-6ddd-4889-9407-cdac09d040a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base_PROMPT_strict = \"\"\"\n",
    "You must answer using ONLY the exact words from the context.\n",
    "\n",
    "Rules:\n",
    "- Do NOT explain.\n",
    "- Do NOT rephrase.\n",
    "- Do NOT add extra information.\n",
    "- Return ONLY the answer phrase.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0049a313-f123-49d0-b0f0-3c0c2f81ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(query, docs, top_n=4):\n",
    "    \"\"\"\n",
    "    query: string\n",
    "    docs: list of (Document, score) from Chroma\n",
    "    \"\"\"\n",
    "\n",
    "    passages = [doc.page_content for doc, _ in docs]\n",
    "    pairs = [(query, passage) for passage in passages]\n",
    "\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "\n",
    "    scored_docs = list(zip(docs, scores))\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # return top_n docs in original (doc, score) format\n",
    "    return [doc for (doc, _orig_score), _ce_score in scored_docs[:top_n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba2b9d9e-6fbd-4f5c-a622-4e65d020b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query_text: str, return_context=False):\n",
    "    results = db.similarity_search_with_score(query_text, k=10)\n",
    "  \n",
    "    reranked_docs = rerank(query_text, results)\n",
    "    \n",
    "    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in reranked_docs])\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_template(Base_PROMPT_strict)\n",
    "    prompt = prompt_template.format(context=context, question=query_text)\n",
    "    #print(prompt)\n",
    "    \n",
    "    response_text = llm.invoke(prompt)\n",
    "    sources = [doc.metadata.get('id', None) for doc in reranked_docs]\n",
    "    formatted_response = f'Response: {response_text}\\n\\nSources: {sources}'\n",
    "    #print(formatted_response)\n",
    "    if return_context:\n",
    "        return response_text, context\n",
    "    return(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "724c3de6-7309-4a59-833b-0588b13006ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A fully-convolutional network that simultaneously predicts object bounds and ratios at that location.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rag(\"What is a RPN?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a750ed1-0404-4dcd-8231-fb5199b9ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "logging.getLogger(\"chromadb\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"sentence_transformers\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"langchain\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a04d3d0-0b04-4cad-9b3c-129e6ea38354",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_HISTORY = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad77c1f1-e8d3-4704-902f-a6c3f9c2f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_chat_history():\n",
    "    CHAT_HISTORY.clear()\n",
    "    print(\"Chat history reset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7432b76c-b95b-40ee-b2a9-37e85aa56bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWRITE_PROMPT = \"\"\"\n",
    "Given the chat history and the latest question, rewrite the question so it is standalone and can be understood without the history.\n",
    "\n",
    "Chat history:\n",
    "{history}\n",
    "\n",
    "Latest question: {question}\n",
    "\n",
    "Standalone question:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c566a7dc-103f-4068-a51e-2848835b03fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query_with_history(query: str, history: list):\n",
    "    if not history:\n",
    "        return query\n",
    "\n",
    "    model = OllamaLLM(model=\"llama3.1\")\n",
    "\n",
    "    history_text = \"\\n\".join(f\"{role}: {msg}\" for role, msg in history[-6:])\n",
    "\n",
    "    prompt = REWRITE_PROMPT.format(history=history_text,question=query,)\n",
    "    #print(prompt)\n",
    "    \n",
    "    rewritten = model.invoke(prompt)\n",
    "    return rewritten.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74e526de-d6de-4500-9102-89136984cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base_PROMPT_2  = \"\"\"\n",
    "Chat history:\n",
    "{history}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer the question based on the above context\n",
    "\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9671db0b-407e-4e71-8500-140fe18cb82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag_hist(query_text: str, history: list, return_context=False, return_sources=False):\n",
    "    standalone_query = rewrite_query_with_history(query_text, history)\n",
    "    \n",
    "    results = db.similarity_search_with_score(standalone_query, k=10)\n",
    "    reranked_docs = rerank(standalone_query, results)\n",
    "\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in reranked_docs])\n",
    "    \n",
    "    history_text = \"\\n\".join(f\"{role}: {msg}\" for role, msg in history[-6:])\n",
    "    prompt_template = ChatPromptTemplate.from_template(Base_PROMPT_2)\n",
    "    prompt = prompt_template.format(context=context, question=query_text, history=history_text,)\n",
    "    #print(prompt)\n",
    "\n",
    "    response_text = llm.invoke(prompt)\n",
    "    history.append((\"user\", query_text))\n",
    "    history.append((\"assistant\", response_text))\n",
    "    sources = [doc.metadata.get('id', None) for doc in reranked_docs]\n",
    "\n",
    "    #print(\"Standalone query:\", standalone_query)\n",
    "    #print(\"\\nResponse: \",response_text)\n",
    "    #print(\"\\nSources:\", sources)\n",
    "    if return_context and return_sources:\n",
    "        return response_text, context, sources\n",
    "    \n",
    "    if return_sources:\n",
    "        return response_text, sources\n",
    "    \n",
    "    if return_context:\n",
    "        return response_text, context\n",
    "    \n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5758520f-be90-4fa8-bfcb-d62475d84a10",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Load pretrained SentenceTransformer: BAAI/bge-large-en-v1.5\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/modules.json \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/README.md \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/modules.json \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/sentence_bert_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/sentence_bert_config.json \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/adapter_config.json \"HTTP/1.1 404 Not Found\"\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/config.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f55a005de6e46bfb3173936fa78e9bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/config.json \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: GET https://huggingface.co/api/models/BAAI/bge-large-en-v1.5/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "INFO: HTTP Request: GET https://huggingface.co/api/models/BAAI/bge-large-en-v1.5/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/1_Pooling/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO: HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/1_Pooling%2Fconfig.json \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: GET https://huggingface.co/api/models/BAAI/bge-large-en-v1.5 \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standalone query: What is an RPN?\n",
      "\n",
      "Response:  According to the abstract:\n",
      "\n",
      "\"An RPN (Region Proposal Network) is a fully-convolutional network that simultaneously predicts object bounds and objectness scores.\"\n",
      "\n",
      "Sources: ['C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Faster-RCNN-Paper.pdf:3:6', 'C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Faster-RCNN-Paper.pdf:5:1', 'C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Faster-RCNN-Paper.pdf:7:2', 'C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Faster-RCNN-Paper.pdf:0:0']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to the abstract:\\n\\n\"An RPN (Region Proposal Network) is a fully-convolutional network that simultaneously predicts object bounds and objectness scores.\"'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rag_hist(\"What is an RPN?\", CHAT_HISTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f36bd2c-f013-4134-8f9a-32785dd076e2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9304878c66c04c479cd6a060088e49f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standalone query: How does an RPN (Region Proposal Network) differ from Fast R-CNN?\n",
      "\n",
      "Response:  According to the abstract and the text, an RPN (Region Proposal Network) differs from Fast R-CNN in that:\n",
      "\n",
      "* An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position.\n",
      "* It shares full-image convolutional features with the detection network, making the region proposal step \"nearly cost-free\".\n",
      "* In contrast, Fast R-CNN relies on pre-computed region proposals, which are then used for detection.\n",
      "\n",
      "Overall, an RPN is a more integrated approach that combines region proposal generation and object detection into a single end-to-end process.\n",
      "\n",
      "Sources: ['C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Faster-RCNN-Paper.pdf:0:0', 'C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Faster-RCNN-Paper.pdf:6:1', 'C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Faster-RCNN-Paper.pdf:7:6', 'C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Faster-RCNN-Paper.pdf:0:1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to the abstract and the text, an RPN (Region Proposal Network) differs from Fast R-CNN in that:\\n\\n* An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position.\\n* It shares full-image convolutional features with the detection network, making the region proposal step \"nearly cost-free\".\\n* In contrast, Fast R-CNN relies on pre-computed region proposals, which are then used for detection.\\n\\nOverall, an RPN is a more integrated approach that combines region proposal generation and object detection into a single end-to-end process.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rag_hist(\"How does it differ from Fast R-CNN?\", CHAT_HISTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2772c60b-f156-4a8e-a043-7ae89b5f7b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee98417c-6f82-46b3-b45d-99791859969e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "575d48ff-e659-4e30-8f4e-99402a13c96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Run PDF -> embeddings -> Chroma pipeline or query RAG system.\")\n",
    "    sub = parser.add_subparsers(dest=\"cmd\")\n",
    "\n",
    "    ingest = sub.add_parser(\"ingest\", help=\"Load PDFs, split, and add to Chroma DB\")\n",
    "    #ingest.add_argument(\"--device\", default=\"cuda\", help=\"device for embeddings model (cpu or cuda)\",)\n",
    "\n",
    "    q = sub.add_parser(\"query\", help=\"Run a RAG query\")\n",
    "    q.add_argument(\"--q\", required=True, help=\"Question to ask\")\n",
    "\n",
    "    reset_chat = sub.add_parser(\"reset-chat\", help=\"Reset the chat history\")\n",
    "\n",
    "    chat = sub.add_parser(\"chat\", help=\"Run conversational RAG with history\")\n",
    "    #chat.add_argument(\"--q\", required=True, help=\"Question to ask\")\n",
    "    # interactive chat, no --q arg needed, will prompt user for input\n",
    "\n",
    "    info = sub.add_parser(\"info\", help=\"Show counts of documents and chunks\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.cmd == \"ingest\":\n",
    "        logging.info(f\"Loading PDFs from {DATA_DIR}\")\n",
    "        docs = load_docs()\n",
    "        docs = filter_pages(docs)\n",
    "        chunks = split_docs(docs)\n",
    "        add_to_chroma(chunks)\n",
    "        logging.info(\"Ingestion complete\")\n",
    "\n",
    "    elif args.cmd == \"query\":\n",
    "        query_rag(args.q)\n",
    "    \n",
    "    elif args.cmd == \"chat\":\n",
    "        print(\"Starting conversational RAG. Type 'exit' to quit.\\n\")\n",
    "        while True:\n",
    "            user_input = input(\"You: \")\n",
    "            if user_input.lower() == \"exit\":\n",
    "                break\n",
    "            query_rag_hist(user_input, CHAT_HISTORY)\n",
    "\n",
    "    elif args.cmd == \"reset-chat\":\n",
    "        CHAT_HISTORY.clear()\n",
    "        logging.info(\"Chat history reset\")\n",
    "\n",
    "    elif args.cmd == \"info\":\n",
    "        docs = load_docs()\n",
    "        docs = filter_pages(docs)\n",
    "        chunks = split_docs(docs)\n",
    "        print(f\"Pages (after filter): {len(docs)}\")\n",
    "        print(f\"Chunks: {len(chunks)}\")\n",
    "\n",
    "    else:\n",
    "        parser.print_help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa01de2-0c56-4660-a9e9-a6e788a9e9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0557f3b-093b-4269-9fa3-c13969c9d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python Code/pipeline.py ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45adc139-7e6e-4205-92ff-6e338eeaf73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python Code/pipeline.py query --q \"what is RPN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb3eff4-9469-460c-bb6d-44b43f094385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c50b561-9711-4011-9cd7-117f8eac9fe4",
   "metadata": {},
   "source": [
    "# UI - Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0d03963-4310-41af-842c-9d4b50d270f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sources(sources):\n",
    "    formatted = []\n",
    "    for s in sources:\n",
    "        try:\n",
    "            path, page, chunk = s.rsplit(\":\", 2)\n",
    "            filename = os.path.basename(path)\n",
    "            formatted.append(f\"• {filename} (page {page})\")\n",
    "        except Exception:\n",
    "            formatted.append(f\"• {s}\")\n",
    "    return \"\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee15e9c0-f8c5-4243-88d6-4487ff6d9ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_fn(message, history):\n",
    "    response, sources = query_rag_hist(message, CHAT_HISTORY, return_sources=True)\n",
    "    #sources_text = \"\\n\".join([f\"• {s}\" for s in sources])\n",
    "    sources_text = format_sources(sources)\n",
    "    final_response = f\"\"\"{response}\n",
    "---\n",
    "### Sources\n",
    "{sources_text}\n",
    "\"\"\"\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25ca6941-8daa-4943-9107-585b67ff0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_pdf(file):\n",
    "    if file is None:\n",
    "        return 'No file Uploaded'\n",
    "    save_path = Path(DATA_DIR)/file.name\n",
    "    shutil.copy(file.name, save_path)\n",
    "\n",
    "    docs = load_docs()\n",
    "    docs = filter_pages(docs)\n",
    "    chunks = split(docs)\n",
    "    add_to_chroma(chunks)\n",
    "\n",
    "    return f'Indexed: {file.name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a165ca07-ebff-4502-9222-ac07c3024166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_chat_ui():\n",
    "    CHAT_HISTORY.clear()\n",
    "    return '', \"Chat reset successfully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9eecf22-e742-4b20-9b6d-825b0aab0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_documents():\n",
    "    files = []\n",
    "\n",
    "    for file in os.listdir(DATA_DIR):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            files.append(file)\n",
    "\n",
    "    if not files:\n",
    "        return \"No documents uploaded.\"\n",
    "\n",
    "    return \"\\n\".join(f\"• {file}\" for file in files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c78507-cf39-4b13-a744-85133c1d3cb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mgr\u001b[49m.Blocks() \u001b[38;5;28;01mas\u001b[39;00m demo:\n\u001b[32m      2\u001b[39m     gr.Markdown(\u001b[33m\"\u001b[39m\u001b[33m# Local RAG PDF QA System\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m gr.Row():\n",
      "\u001b[31mNameError\u001b[39m: name 'gr' is not defined"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Local RAG PDF QA System\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale = 3):\n",
    "            chatbot = gr.ChatInterface(fn=chat_fn,\n",
    "                                       title=\"Ask questions about your PDFs\")\n",
    "            \n",
    "            gr.Markdown(\"### Example Questions\")\n",
    "            gr.Examples(examples = [\"What was David Goggins pullup record?\",\n",
    "                                    \"How many Hell Weeks did Goggins complete?\",\n",
    "                                    \"What is Faster R-CNN?\",\n",
    "                                    \"What is ROI pooling used for?\",],\n",
    "                                    inputs = chatbot.textbox)\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"## Documents\")\n",
    "            doc_list = gr.Textbox(value=list_documents(), \n",
    "                                  label=\"Indexed Documents\",\n",
    "                                  interactive=False,\n",
    "                                  lines=4)\n",
    "            gr.Markdown(\"## Upload new PDF\")\n",
    "            file_upload = gr.File(file_types = ['.pdf'],label = 'Select PDF')\n",
    "            upload_btn = gr.Button(\"Add Document\")\n",
    "            upload_status = gr.Textbox(label = 'Upload Status')\n",
    "            \n",
    "            reset_btn = gr.Button(\"Reset Chat\", variant=\"secondary\")\n",
    "            reset_status = gr.Markdown()\n",
    "            \n",
    "    reset_btn.click(fn=reset_chat_ui, outputs=[chatbot.textbox, reset_status])\n",
    "    upload_btn.click(fn=upload_pdf, inputs=file_upload, outputs=upload_status).then(fn=list_documents, outputs=doc_list)\n",
    "\n",
    "demo.launch(inline = False, inbrowser=True)# share=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f41a544-4495-4ecc-9143-0938fbc6618e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
