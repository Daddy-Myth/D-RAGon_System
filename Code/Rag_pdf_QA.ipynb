{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22712215-bc7c-4a52-ae5d-ff2945b256b2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Notebook Desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91b332c-23d8-4d8b-ac74-c88716992406",
   "metadata": {},
   "source": [
    "## üìí RAG PDF QA Development Notebook\n",
    "\n",
    "This notebook contains the **experimental, prototyping, and evaluation work** for building a local Retrieval-Augmented Generation (RAG) system over PDF documents.\n",
    "\n",
    "It serves as the **primary development environment** for testing components, validating retrieval quality, measuring system performance, and refining architecture before migrating stable logic into production `.py` modules.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What This Notebook Includes\n",
    "\n",
    "### üîπ PDF Ingestion and Preprocessing\n",
    "- Loading PDFs from disk using LangChain loaders  \n",
    "- Page-level metadata extraction  \n",
    "- Boilerplate filtering (copyright, TOC, publisher pages)  \n",
    "- Document inspection and debugging  \n",
    "\n",
    "### üîπ Text Chunking and Index Preparation\n",
    "- Recursive character-based chunking experiments  \n",
    "- Chunk size and overlap tuning  \n",
    "- Low-information chunk removal  \n",
    "- Page-aligned chunk ID generation  \n",
    "- Chunk integrity verification  \n",
    "\n",
    "### üîπ Embedding and Vector Storage\n",
    "- Initializing and testing `BAAI/bge-large-en-v1.5` embeddings  \n",
    "- Local embedding generation (CPU/GPU)  \n",
    "- Creating and persisting Chroma vector database  \n",
    "- Incremental indexing and duplicate prevention  \n",
    "- Metadata validation and index inspection  \n",
    "\n",
    "### üîπ Retrieval and Re-Ranking Experiments\n",
    "- Top-K vector similarity retrieval testing  \n",
    "- Retrieval quality debugging using source page IDs  \n",
    "- Cross-encoder re-ranking integration  \n",
    "- Testing `cross-encoder/ms-marco-MiniLM-L-6-v2`  \n",
    "- Retrieval precision vs recall tuning  \n",
    "\n",
    "### üîπ Prompt Engineering and LLM Integration\n",
    "- RAG prompt design and refinement  \n",
    "- Context grounding and hallucination control  \n",
    "- Strict extraction-based answer prompts  \n",
    "- Integration with Llama-3.1 via Ollama  \n",
    "- Stateless and conversational prompt variants  \n",
    "\n",
    "### üîπ Evaluation Pipeline Development\n",
    "- Creation of manually curated QA evaluation datasets  \n",
    "- Ground-truth source page annotation  \n",
    "- Retrieval evaluation using Recall@K  \n",
    "- End-to-end answer accuracy measurement  \n",
    "- Hallucination rate measurement  \n",
    "- Latency benchmarking  \n",
    "- Failure case analysis and debugging  \n",
    "\n",
    "### üîπ UI and Pipeline Integration Testing\n",
    "- Testing pipeline functions before UI integration  \n",
    "- Validation of conversational query rewriting  \n",
    "- Source attribution verification  \n",
    "- Document upload and incremental indexing testing  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è What This Notebook Is *Not*\n",
    "\n",
    "This notebook is **not the production entrypoint**.\n",
    "\n",
    "It does not serve as:\n",
    "\n",
    "- ‚ùå Final pipeline executable  \n",
    "- ‚ùå Gradio UI implementation  \n",
    "- ‚ùå CLI entry script  \n",
    "- ‚ùå Modular backend service  \n",
    "\n",
    "Production logic has been migrated into dedicated modules:\n",
    "\n",
    "- `Updated_pipeline.py` ‚Üí Core RAG pipeline  \n",
    "- `app.py` ‚Üí Gradio UI application  \n",
    "- `Chroma/` ‚Üí Persistent vector database  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Purpose\n",
    "\n",
    "This notebook exists to:\n",
    "\n",
    "- Prototype and validate system components  \n",
    "- Tune chunking, retrieval, and reranking  \n",
    "- Develop and validate evaluation metrics  \n",
    "- Debug retrieval and generation failures  \n",
    "- Benchmark system performance  \n",
    "- Test new ideas before production integration  \n",
    "\n",
    "It serves as the **development and experimentation environment**, while `.py` modules provide the stable, deployable implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a1c93-b277-425f-9447-fdac55b740cc",
   "metadata": {},
   "source": [
    "# Lib Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95cfd65f-239d-4502-92c8-be3b7ab3dab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install langchain langchain-community\n",
    "#!pip install pypdf\n",
    "#!pip install sentence-transformers\n",
    "#!pip install chromadb\n",
    "#!pip install langchain-chroma\n",
    "#!pip install -U langchain-ollama\n",
    "#!pip install sentence-transformers\n",
    "#!pip install tqdm\n",
    "#!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da1e05-9169-47b6-ba2c-d05f79bc33b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "699152a5-c552-4934-a69c-2df154d4f879",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "65a6a5ac-26fc-470d-a787-3f52be381f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "from sentence_transformers import CrossEncoder\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gradio as gr\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3fc2fa-3bce-4794-aebc-3c2d6b9f2d12",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a92ff19a-5e3c-4fa7-a1b8-5bd2fc5a9e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT =Path(r\"C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\")\n",
    "DATA_DIR = ROOT/'Data'\n",
    "CHROMA_DIR = ROOT/'Chroma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "220cfb3d-e134-49d7-bd57-8d8461c0b0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Data\n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Chroma\n"
     ]
    }
   ],
   "source": [
    "print(ROOT)\n",
    "print(DATA_DIR)\n",
    "print(CHROMA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a4016-c955-421f-a33f-940f61074d70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PDF Ingession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27f4b766-cd51-4265-8ab5-abec895ab4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs():\n",
    "    doc_loader = PyPDFDirectoryLoader(DATA_DIR)\n",
    "    return doc_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ec852b1-4ebd-49ee-b0b3-db7c8953ba42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='To the unrelenting voice in my head that will never allow me to stop.' metadata={'producer': 'calibre (2.85.1) [https://calibre-ebook.com]', 'creator': 'calibre (2.85.1) [https://calibre-ebook.com]', 'creationdate': '2020-06-25T21:00:51+00:00', 'author': 'David Goggins', 'moddate': '2020-06-25T21:01:00+00:00', 'title': \"Can't Hurt Me: Master Your Mind and Defy the Odds\", 'source': 'C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Can_t-Hurt-Me-David-Goggins.pdf', 'total_pages': 303, 'page': 2, 'page_label': '3'}\n"
     ]
    }
   ],
   "source": [
    "docs = load_docs()\n",
    "print(docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "905f47e8-75fa-4d72-b6b4-1a6d55c8fbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47465248-21e1-4e01-a078-48b7b4913739",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8053b7b5-cd75-40f7-9e3e-422ff27ea21f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Page level Filtering...  Removing Boilerplate and short pdf pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f505803c-ec3b-40a5-a367-b02304bad4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pages(docs, min_chars = 200):\n",
    "    cleaned = []\n",
    "    blacklist = [\n",
    "        \"all rights reserved\",\n",
    "        \"copyright\",\n",
    "        \"isbn\",\n",
    "        \"table of contents\"\n",
    "    ]\n",
    "    for d in docs:\n",
    "        text = d.page_content.lower()\n",
    "\n",
    "        if len(text)<min_chars: #removes short pages\n",
    "            continue\n",
    "        if any(b in text for b in blacklist): # removes boilerplate pages\n",
    "            continue\n",
    "        cleaned.append(d)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d05bb4c-7480-47ea-bcf4-841618048840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_docs(docs: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 800,\n",
    "        chunk_overlap = 80,\n",
    "        length_function = len, \n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks =  text_splitter.split_documents(docs)\n",
    "     # dropping small chunks >200 chars\n",
    "    chunks = [c for c in chunks if len(c.page_content)>200]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d47bfab7-d762-4f12-877c-760dff93c574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='CONTENTS\n",
      "INTRODUCTION\n",
      "1. I SHOULD HAVE BEEN A STATISTIC\n",
      "2. TRUTH HURTS\n",
      "3. THE IMPOSSIBLE TASK\n",
      "4. TAKING SOULS\n",
      "5. ARMORED MIND\n",
      "6. IT‚ÄôS NOT ABOUT A TROPHY\n",
      "7. THE MOST POWERFUL WEAPON\n",
      "8. TALENT NOT REQUIRED\n",
      "9. UNCOMMON AMONGST UNCOMMON\n",
      "10. THE EMPOWERMENT OF FAILURE\n",
      "11. WHAT IF?\n",
      "ACKNOWLEDGMENTS\n",
      "ABOUT THE AUTHOR' metadata={'producer': 'calibre (2.85.1) [https://calibre-ebook.com]', 'creator': 'calibre (2.85.1) [https://calibre-ebook.com]', 'creationdate': '2020-06-25T21:00:51+00:00', 'author': 'David Goggins', 'moddate': '2020-06-25T21:01:00+00:00', 'title': \"Can't Hurt Me: Master Your Mind and Defy the Odds\", 'source': 'C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Can_t-Hurt-Me-David-Goggins.pdf', 'total_pages': 303, 'page': 3, 'page_label': '4'}\n"
     ]
    }
   ],
   "source": [
    "docs = load_docs()\n",
    "docs = filter_pages(docs)\n",
    "\n",
    "chunks = split_docs(docs)\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13367d3f-ef2e-4619-a903-19941c3f63e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299\n",
      "959\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e31db6d-cb97-4577-9b45-cdebd798a9ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Calculating chunk IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a076415-c77c-427c-abb9-6cba6a4f6e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_chunk_ids(chunks):\n",
    "    # This will create IDs like \"data/monopoly.pdf:6:2\"\n",
    "    # Page Source : Page Number : Chunk Index\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5357a5-5d14-4a01-aa44-c49e4d8a927d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "747924b4-1e37-439b-82c5-0a798648916f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Data\\Can_t-Hurt-Me-David-Goggins.pdf:6:0\n"
     ]
    }
   ],
   "source": [
    "test = calc_chunk_ids([chunks[5]])\n",
    "print(test[0].metadata[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2628020b-42ef-4d80-8e01-f9ab37854182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Data\\Can_t-Hurt-Me-David-Goggins.pdf:3:0\n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Data\\Can_t-Hurt-Me-David-Goggins.pdf:4:0\n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Data\\Can_t-Hurt-Me-David-Goggins.pdf:4:1\n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Data\\Can_t-Hurt-Me-David-Goggins.pdf:5:0\n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Data\\Can_t-Hurt-Me-David-Goggins.pdf:5:1\n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Data\\Can_t-Hurt-Me-David-Goggins.pdf:6:0\n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Data\\Can_t-Hurt-Me-David-Goggins.pdf:6:1\n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Data\\Can_t-Hurt-Me-David-Goggins.pdf:6:2\n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Data\\Can_t-Hurt-Me-David-Goggins.pdf:7:0\n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Data\\Can_t-Hurt-Me-David-Goggins.pdf:7:1\n"
     ]
    }
   ],
   "source": [
    "test = calc_chunk_ids(chunks[:10])\n",
    "for c in test:\n",
    "    print(c.metadata[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3743c779-65ec-4a82-bf45-5a0ddc72e32c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Embeddings Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "0eb44075-4eef-4a96-8294-ec9b167c4e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_function(device: str = None):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    embeddings = HuggingFaceBgeEmbeddings(\n",
    "        model_name = 'BAAI/bge-large-en-v1.5',\n",
    "        model_kwargs={'device':'cuda'},\n",
    "        encode_kwargs={'normalize_embeddings':True},\n",
    "        query_instruction= \"Represent this sentence for searching relevant passages:\"\n",
    "    )\n",
    "    return embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a6163-c9cb-4c03-b35c-de66bf4f614a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46986542-1844-467e-9f05-b95c157d3693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbea5b6761834e208352d5df047a2ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "emb = get_embeddings_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9334a92-5658-4c71-acc8-510318909ef8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "[0.048546068370342255, 0.005149699281901121, -0.02971961721777916, 0.01099423784762621, 0.03847793862223625, 0.0020174733363091946, 0.0021198869217187166, -0.04009025916457176, -0.016631048172712326, 0.0679197907447815]\n"
     ]
    }
   ],
   "source": [
    "vec = emb.embed_query(\"What is Faster R-CNN?\")\n",
    "print(len(vec))  # 1024 dim embedding\n",
    "print(vec[:10])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7612fa61-6983-4185-b798-afed861226b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "test = chunks[0]\n",
    "test_vecs = emb.embed_documents([test.page_content])\n",
    "print(len(test_vecs[0]))\n",
    "#print(test_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7015dbf8-39d4-43a4-9d56-bd225d45f62c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a4e0a-7647-49bd-88ee-4bad15528357",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b4723e7c-d2a6-4ecd-bfe2-5640f7ab68b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdea08dd67d43c7b80b185592f37847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "# load DB\n",
    "db = Chroma(persist_directory=CHROMA_DIR,\n",
    "            embedding_function = get_embeddings_function())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "910b1180-2df8-4423-8dc9-fbb946710523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calc Page IDs \n",
    "chunks_with_ids = calc_chunk_ids(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6a110b9e-47cd-4193-aa7c-3a0cce12b748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of existing dicuments in DB: 0\n"
     ]
    }
   ],
   "source": [
    "# Add or update the documents\n",
    "existing_items = db.get(include=[])\n",
    "existing_ids = set(existing_items['ids'])\n",
    "print(f\"Number of existing dicuments in DB: {len(existing_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "44c4b228-49fe-46bf-85b5-060a00ddf6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding new documents: 959\n"
     ]
    }
   ],
   "source": [
    "# Only add docs that don't exist in the DB\n",
    "new_chunks = []\n",
    "for chunk in chunks_with_ids:\n",
    "    if chunk.metadata['id'] not in existing_ids:\n",
    "        new_chunks.append(chunk)\n",
    "\n",
    "if len(new_chunks):\n",
    "    print(f'Adding new documents: {len(new_chunks)}')\n",
    "    new_chunk_ids = [chunk.metadata['id'] for chunk in new_chunks]\n",
    "    db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "    #db.persist()\n",
    "else:\n",
    "    print(\"No New Documents to add\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4955a7de-4962-4087-87f1-c52e1f19c9a8",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "626d6940-7feb-4086-8d9f-e5c85fe504c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_chroma(chunks: list[Document]):\n",
    "    # load DB\n",
    "    db = Chroma(persist_directory=CHROMA_DIR,\n",
    "                embedding_function = get_embeddings_function())\n",
    "\n",
    "    #Calc Page IDs \n",
    "    chunks_with_ids = calc_chunk_ids(chunks)\n",
    "\n",
    "    # Add or update the documents\n",
    "    existing_items = db.get(include=[])\n",
    "    existing_ids = set(existing_items['ids'])\n",
    "    print(f\"Number of existing dicuments in DB: {len(existing_ids)}\")\n",
    "\n",
    "    # Only add docs that don't exist in the DB\n",
    "    new_chunks = []\n",
    "    for chunk in chunks_with_ids:\n",
    "        if chunk.metadata['id'] not in existing_ids:\n",
    "            new_chunks.append(chunk)\n",
    "    \n",
    "    if len(new_chunks):\n",
    "        print(f'Adding new documents: {len(new_chunks)}')\n",
    "        new_chunk_ids = [chunk.metadata['id'] for chunk in new_chunks]\n",
    "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "        #db.persist()\n",
    "    else:\n",
    "        print(\"No New Documents to add\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076fea72-09c8-4365-9c2b-1bc71acb0c8f",
   "metadata": {},
   "source": [
    "## Loading DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "242d31a9-dfc8-43a7-88e6-1a339b4565a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Archit\\AppData\\Local\\Temp\\ipykernel_44224\\222130469.py:2: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceBgeEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dcfb2fb83bc4bb2952ba338f013800d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "959\n"
     ]
    }
   ],
   "source": [
    "db = Chroma(\n",
    "    persist_directory=\"chroma\",\n",
    "    embedding_function=get_embeddings_function())\n",
    "\n",
    "print(db._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bbb2c2-26ea-420f-a235-4b8441dc8d9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## To clear db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38338a1d-a1f1-4e5c-b659-a6bdbcf27e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deletes the on-disk Chroma directory so the vector DB can be rebuilt from scratch.\n",
    "# def clear_database():\n",
    "#     if os.path.exists(CHROMA_PATH):\n",
    "#         shutil.rmtree(CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24209aea-296b-42ce-aeeb-ddab388662b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Retriever Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e9ff1237-67f2-477b-a6ce-aeae3433b1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5373e9ec8543b8912570452bf552e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220861b9a7ce4524b99c75635d338f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Initialize Embeddings\n",
    "emb_fxn = get_embeddings_function()\n",
    "\n",
    "# Initialize Vector Store\n",
    "db = Chroma(\n",
    "    persist_directory=CHROMA_DIR,\n",
    "    embedding_function=emb_fxn\n",
    ")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OllamaLLM(model='llama3.1')\n",
    "\n",
    "# Cross encoder\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf29132-916f-4797-ba27-042e33f8b77d",
   "metadata": {},
   "source": [
    "Using Top-k Similarity Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7cbe640c-def3-45ce-935a-a355d8a7e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base_PROMPT = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "45794902-4bc2-4ba4-96a8-dde6c0107b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base_PROMPT_strict = \"\"\"\n",
    "You must answer using ONLY the exact words from the context.\n",
    "\n",
    "Rules:\n",
    "- Do NOT explain.\n",
    "- Do NOT rephrase.\n",
    "- Do NOT add extra information.\n",
    "- Return ONLY the answer phrase.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a38f1f-ee09-45c2-ba04-1b6744c9243e",
   "metadata": {},
   "source": [
    "query_rag(query_text)  \n",
    "    ‚Üí embed query  \n",
    "    ‚Üí similarity_search  \n",
    "    ‚Üí build prompt  \n",
    "    ‚Üí LLM   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f28f32-9299-41b3-ac38-284adbebec85",
   "metadata": {},
   "source": [
    "### Old but working function with no reranker only top k=45 similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c2e40a49-eb04-4530-87e0-12e4402335ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def query_rag(query_text: str):\n",
    "#     emb_fxn = get_embeddings_function()\n",
    "#     db = Chroma(persist_directory=CHROMA_DIR,embedding_function = emb_fxn)\n",
    "\n",
    "#     results = db.similarity_search_with_score(query_text, k=4)\n",
    "\n",
    "#     context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "#     prompt_template = ChatPromptTemplate.from_template(Base_PROMPT)\n",
    "#     prompt = prompt_template.format(context=context, question=query_text)\n",
    "#     #print(prompt)\n",
    "\n",
    "#     model = OllamaLLM(model = 'llama3.1')\n",
    "#     response_text = model.invoke(prompt)\n",
    "\n",
    "#     sources = [doc.metadata.get('id', None) for doc, _source in results]\n",
    "#     formatted_response = f'Response: {response_text}\\n\\nSources: {sources}'\n",
    "#     #print(formatted_response)\n",
    "#     return(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df147412-38b9-4dca-a399-d933d6f34b6a",
   "metadata": {},
   "source": [
    "## Re-Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "66e3cb50-7905-4f72-9316-30064217ba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(query, docs, top_n=4):\n",
    "    \"\"\"\n",
    "    query: string\n",
    "    docs: list of (Document, score) from Chroma\n",
    "    \"\"\"\n",
    "\n",
    "    passages = [doc.page_content for doc, _ in docs]\n",
    "    pairs = [(query, passage) for passage in passages]\n",
    "\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "\n",
    "    scored_docs = list(zip(docs, scores))\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # return top_n docs in original (doc, score) format\n",
    "    return [doc for (doc, _orig_score), _ce_score in scored_docs[:top_n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e52a322-7d05-4ddb-aca0-868a3a065a48",
   "metadata": {},
   "source": [
    "### new fxn with reranker and top k=4 similarity search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "b911084c-3ce7-42d3-9947-c7a6aefc137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query_text: str, return_context=False):\n",
    "    results = db.similarity_search_with_score(query_text, k=10)\n",
    "  \n",
    "    reranked_docs = rerank(query_text, results)\n",
    "    \n",
    "    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in reranked_docs])\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_template(Base_PROMPT_strict)\n",
    "    prompt = prompt_template.format(context=context, question=query_text)\n",
    "    #print(prompt)\n",
    "    \n",
    "    response_text = llm.invoke(prompt)\n",
    "    sources = [doc.metadata.get('id', None) for doc in reranked_docs]\n",
    "    formatted_response = f'Response: {response_text}\\n\\nSources: {sources}'\n",
    "    #print(formatted_response)\n",
    "    if return_context:\n",
    "        return response_text, context\n",
    "    return(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a034ad-5c0b-4337-9897-caa757d8f583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "4d66a276-72fc-4a78-8a95-4f1b40c65135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nearly 300 pounds'"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test= \"What was David Goggins max weight?\"\n",
    "query_rag(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e80fc1ae-d69c-404f-b4f6-94166e84eb10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the text, Goggins did 2 Hell Weeks, but he also \"participated\" in 3 Hell Weeks. It\\'s not clear what this means, but it seems that he was present during 3 Hell Weeks as either a student or an instructor, rather than being specifically tested through Hell Week himself.'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test= \"How many hell weeks did Goggins do ?\"\n",
    "query_rag(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5e77c386-3390-46a3-9dc4-794741cc4abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Region Proposal Network (RPN) is a fully-convolutional network that simultaneously predicts object bounds and ratios at a location. It can be trained end-to-end specifically for generating detection proposals, and it shares full-image convolutional features with the detection network.'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test= \"What is a RPN?\"\n",
    "query_rag(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4127194a-f518-43f9-8968-fa0010dad4a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f88a70-1eed-4b88-9c1d-3cd8add933bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Query Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "3bdceb8a-8706-489c-b032-32ba2dfc691c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2642dcb54ae94cef8bebdf78df8ac36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90275b7735547ff91764052171b1831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "emb_fxn = get_embeddings_function()\n",
    "db = Chroma(persist_directory=\"chroma\",embedding_function=get_embeddings_function())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "600b45b7-ca1c-455a-9a4f-f815ce73fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"When was Goggins' first Badwater? and how many did he run?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1d3135eb-6c1d-4ab7-a522-aa0a2cddb94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = db.similarity_search_with_score(query_text, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "70e4651e-f676-468c-a424-3bffdb153ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "#print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "2de10993-1cce-4556-9dcb-b7b059304cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(Base_PROMPT)\n",
    "#print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "6e2ba318-3ad3-4173-951b-a59ad29871cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "around and through it so it would not derail me. By the time I toed up to the\n",
      "line at Badwater at 6 a.m. on July 22, 2006, I‚Äôd moved my governor to 80\n",
      "percent. I‚Äôd doubled my ceiling in six months, and you know what that\n",
      "guaranteed me?\n",
      "Jack fucking shit.\n",
      "Badwater has a staggered start. Rookies started at 6 a.m., veteran runners\n",
      "had an 8 a.m. start, and the true contenders wouldn‚Äôt take off until 10 a.m.,\n",
      "which put them in Death Valley for peak heat. Chris Kostman was one\n",
      "hilarious son of a bitch. But he didn‚Äôt know he‚Äôd given one hard\n",
      "motherfucker a serious tactical advantage. Not me. I‚Äôm talking about Akos\n",
      "Konya.\n",
      "Akos and I met up the night before at the Furnace Creek Inn, where all the\n",
      "athletes stayed. He was a first-timer too, and he looked a hell of a lot better\n",
      "\n",
      "---\n",
      "\n",
      "garb. I preferred to go incognito. I was the shadow figure filtering into a new\n",
      "world of pain.\n",
      "During my first Badwater\n",
      "Although Akos set a fast pace, the heat didn‚Äôt bother me, partly because it\n",
      "was early and because I‚Äôd heat trained so well. We were the two best runners\n",
      "in the 6 a.m. group by far, and when we passed the Furnace Creek Inn at\n",
      "8:40 a.m., some of the runners from the 10 a.m. group were outside,\n",
      "including Scott Jurek, the defending champion, Badwater record-holder, and\n",
      "an ultra legend. He must have known we were making great time, but I‚Äôm\n",
      "not sure he realized that he‚Äôd just glimpsed his stiffest competition.\n",
      "Not long after, Akos put some space between us, and at mile twenty-six, I\n",
      "started to realize that, once again, I went out way too fast. I was dizzy and\n",
      "\n",
      "---\n",
      "\n",
      "CHAPTER ELEVEN\n",
      "11. WHAT IF?\n",
      "B EFORE  THE  RACE  EVEN  KICKED  OFF  I KNEW  I WAS  FUCKED . I N  2014, THE\n",
      "National Park Service wouldn‚Äôt approve the traditional Badwater course, so\n",
      "Chris Kostman redrew the map. Instead of starting in Death Valley National\n",
      "Park and running forty-two miles through the hottest desert on the planet, it\n",
      "would launch further upcountry at the base of a twenty-two-mile climb. That\n",
      "wasn‚Äôt my problem. It was the fact that I toed the line eleven pounds over\n",
      "my usual race weight, and had gained ten of those pounds in the previous\n",
      "seven days. I wasn‚Äôt a fat ass. To the average eye I looked fit, but Badwater\n",
      "wasn‚Äôt an average race. To run and finish strong, my condition needed to be\n",
      "tip top, and I was far from it. Whatever was happening to me came as a\n",
      "\n",
      "---\n",
      "\n",
      "shoulder length hair corralled with a bandana, otherwise their uniform was\n",
      "identical. Jurek was the mule and Olson was riding him.\n",
      "‚ÄúCome on, Jurker! Come on, Jurker! This is your race,‚Äù Olson said as they\n",
      "passed me up. ‚ÄúNo one is better than you! No one!‚Äù Olson kept talking as\n",
      "they ran ahead, reminding Jurek that he had more to give. Jurek obliged and\n",
      "kept charging up the mountain. He left it all out on that unforgiving asphalt.\n",
      "It was amazing to watch.\n",
      "Jurek wound up winning the 2006 edition of Badwater when he finished in\n",
      "twenty-five hours and forty-one minutes, seventeen minutes faster than\n",
      "Akos, who must have regretted his power nap, but that wasn‚Äôt my concern. I\n",
      "had a race of my own to finish.\n",
      "Whitney Portal Road winds up a parched, exposed rock escarpment for ten\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: When was Goggins' first Badwater? and how many did he run?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(context=context, question=query_text)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88868185-ab41-4c50-a1b7-38f1203d3ead",
   "metadata": {},
   "source": [
    "### Invoiking the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8608ce5e-efc1-4402-bd2c-994896611c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaLLM(model = 'llama3.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "44dca887-42d0-410f-b836-06f528cbca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8066d1a0-6503-4101-a0ea-6e1b303a1767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text:\n",
      "\n",
      "* The date of David Goggins' first Badwater is not explicitly mentioned in the given snippet. However, it is mentioned that \"During my first Badwater\" refers to an event that occurred around July 22, 2006 (as mentioned in Chapter Eleven).\n",
      "* It is implied that Goggins ran more than one Badwater, as he mentions running a second Badwater in 2014 and seems to be reminiscing about his previous experiences.\n"
     ]
    }
   ],
   "source": [
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "3064b1ca-d443-4a35-933c-8fbcc296c77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Based on the provided text:\n",
      "\n",
      "* The date of David Goggins' first Badwater is not explicitly mentioned in the given snippet. However, it is mentioned that \"During my first Badwater\" refers to an event that occurred around July 22, 2006 (as mentioned in Chapter Eleven).\n",
      "* It is implied that Goggins ran more than one Badwater, as he mentions running a second Badwater in 2014 and seems to be reminiscing about his previous experiences.\n",
      "\n",
      "Sources: ['C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Can_t-Hurt-Me-David-Goggins.pdf:182:1', 'C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Can_t-Hurt-Me-David-Goggins.pdf:183:0', 'C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Can_t-Hurt-Me-David-Goggins.pdf:277:0', 'C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Can_t-Hurt-Me-David-Goggins.pdf:186:0']\n"
     ]
    }
   ],
   "source": [
    "sources = [doc.metadata.get('id', None) for doc, _source in results]\n",
    "formatted_response = f'Response: {response_text}\\n\\nSources: {sources}'\n",
    "print(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b9350a0d-e36a-4099-bde1-ee1350aa2f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(prompt + '\\n' + formatted_response) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6bc79b-1879-446e-b205-2ad432ecbea3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Testing Rag OP Using Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7ebc586e-b15f-432b-80cd-9d3cc583adcd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# EVAL_PROMPT = \"\"\"\n",
    "# Expected Response: {expected_response}\n",
    "# Actual Response: {actual_response}\n",
    "# ---\n",
    "# Does the actual actual response mean the same as the expected response?\n",
    "# (Answer with 'true' or 'false')\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e2eced17-7fc1-4206-931e-2cda8e0a0665",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# EVAL_PROMPT = \"\"\"\n",
    "# Expected Answer:\n",
    "# {expected_response}\n",
    "\n",
    "# Model Answer:\n",
    "# {actual_response}\n",
    "\n",
    "# ---\n",
    "\n",
    "# Decide whether the model answer contains the same core factual information as the expected answer.\n",
    "\n",
    "# Ignore wording differences, extra commentary, or stylistic changes.\n",
    "# Focus only on whether the main fact(s) match.\n",
    "\n",
    "# Reply with exactly one word: true or false.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6c514dad-3682-49a7-93f6-9345ae6325e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_PROMPT = \"\"\"\n",
    "Expected Answer:\n",
    "{expected_response}\n",
    "\n",
    "Model Answer:\n",
    "{actual_response}\n",
    "\n",
    "---\n",
    "\n",
    "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
    "\n",
    "Rules:\n",
    "- If any number, count, or entity differs, answer false.\n",
    "- If the Expected Answer is contradicted, answer false.\n",
    "- Do NOT be generous.\n",
    "- Do NOT infer or reinterpret.\n",
    "- Ignore wording only when the facts clearly match.\n",
    "\n",
    "Respond with exactly one word: true or false.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d0c3e-8982-41ab-a522-c7fd2dbc3947",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "746a098e-e005-4087-8549-6e39fec2c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What is a RPN?'\n",
    "expected_response= 'A fully convolutional network that predicts object bounding boxes and objectness scores from shared feature maps to generate region proposals'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ab85cdbb-1cf9-48e2-87a6-923828eeff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"How many hell weeks did Goggins do ?\"\n",
    "expected_response=\"He did three Hell Weeks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "aed2aeca-217e-451e-980c-c1a4eae06245",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What dataset was used?'\n",
    "expected_response = 'PASCAL VOC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d7404fab-248e-4ea3-b741-1d40af4d4b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1ac17d77ad47ed8d7c25a95248a773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "PASCAL VOC\n",
      "\n",
      "Model Answer:\n",
      "According to Table 2 and Table 3, the datasets used were:\n",
      "\n",
      "* PASCAL VOC 2007 test set\n",
      "* PASCAL VOC 2012 test set\n",
      "\n",
      "Additionally, the training data for some experiments included:\n",
      "\n",
      "* \"07\": VOC 2007 trainval\n",
      "* \"07+12\": union set of VOC 2007 trainval and VOC 2012 trainval\n",
      "\n",
      "---\n",
      "\n",
      "Decide whether the model answer contains the same core factual information as the expected answer.\n",
      "\n",
      "Ignore wording differences, extra commentary, or stylistic changes.\n",
      "Focus only on whether the main fact(s) match.\n",
      "\n",
      "Reply with exactly one word: true or false.\n",
      "\n",
      "\u001b[92mResponse: true\u001b[0m\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "response_text = query_rag(question)\n",
    "prompt = EVAL_PROMPT.format(expected_response = expected_response, actual_response = response_text)\n",
    "\n",
    "model = OllamaLLM(model = \"llama3.1\")\n",
    "eval_results_str = model.invoke(prompt)\n",
    "eval_results_str_cleaned = eval_results_str.strip().lower()\n",
    "\n",
    "print(prompt)\n",
    "if \"true\" in eval_results_str_cleaned:\n",
    "    print(\"\\033[92m\" + f\"Response: {eval_results_str_cleaned}\" + \"\\033[0m\")\n",
    "    print('True')\n",
    "elif 'false' in eval_results_str_cleaned:\n",
    "    print(\"\\033[91m\" + f\"Response: {eval_results_str_cleaned}\" + \"\\033[0m\")\n",
    "    print('False') \n",
    "else:\n",
    "    raise ValueError(f'Invalid evaluation result. Cannot determine if \"True\" or \"False\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdcfb94-9cb3-4208-a4bb-eab03a6f3355",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2bc99be5-abf8-455a-8eea-3c7b32c3528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_and_validate(question: str, expected_response: str):\n",
    "    response_text = query_rag(question)\n",
    "    prompt = EVAL_PROMPT.format(expected_response = expected_response, actual_response = response_text)\n",
    "    \n",
    "    model = OllamaLLM(model = \"llama3.1\")\n",
    "    eval_results_str = model.invoke(prompt)\n",
    "    eval_results_str_cleaned = eval_results_str.strip().lower()\n",
    "    \n",
    "    print(prompt)\n",
    "    if \"true\" in eval_results_str_cleaned:\n",
    "        print(\"\\033[92m\" + f\"Response: {eval_results_str_cleaned}\" + \"\\033[0m\")\n",
    "        print('True')\n",
    "        return True\n",
    "    elif 'false' in eval_results_str_cleaned:\n",
    "        print(\"\\033[91m\" + f\"Response: {eval_results_str_cleaned}\" + \"\\033[0m\")\n",
    "        print('False')\n",
    "        return False\n",
    "    else:\n",
    "        raise ValueError(f'Invalid evaluation result. Cannot determine if \"True\" or \"False\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ac1ae-88e5-4cd2-ba27-a2a78e651cd1",
   "metadata": {},
   "source": [
    "### More single question tests on the funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "35bf324d-6ecb-43c0-b930-2a7947e2a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Cant_hurt_me():\n",
    "    assert query_and_validate(\n",
    "        question=\"How many hell weeks did Goggins do ?\",\n",
    "        expected_response=\"He did 3 Hell Weeks\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "3810fc21-b1c5-4e48-a3a2-2b532a770203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a5d46ead10480abe0ac9a41ab8c6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "He did 3 Hell Weeks\n",
      "\n",
      "Model Answer:\n",
      "According to the text, Goggins survived two Hell Weeks (not as a student) and participated in three. However, it also mentions that \"After surviving two Hell Weeks\", implying that he was a participant/student at least twice, which would be his two participations mentioned earlier, but not necessarily as a survivor.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[92mResponse: true\u001b[0m\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "test_Cant_hurt_me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "359e2177-3c40-46de-95e0-f45ff97b4080",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3a5a9a2b744954a1c68fd4913a1ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "ankles had vanished‚Ä¶because my feet had swollen enough to stabilize those\n",
      "tendons. Was this a good thing long-term? Probably not, but you take what\n",
      "you can get on the ultra circuit, where you have to roll with whatever gets\n",
      "you from mile to mile. Meanwhile, my quads and calves ached like they‚Äôd\n",
      "been thumped with a sledgehammer. Yeah, I had done a lot of running, but\n",
      "most of it‚Äîincluding my ruck runs‚Äîon pancake flat terrain in San Diego,\n",
      "not on slick jungle trails.\n",
      "Kate was waiting for me by the time I completed my second lap, and after\n",
      "spending a relaxing morning on Waikiki beach, she watched in horror as I\n",
      "materialized from the mist like a zombie from the Walking Dead. I sat and\n",
      "guzzled as much water as I could. By then, word had gotten out that it was\n",
      "my first trail race.\n",
      "\n",
      "---\n",
      "\n",
      "one dragged out like an elastic thread, sending shockwaves of pain from my\n",
      "toes to the space behind my eyeballs. I hacked and coughed, my gut\n",
      "twisted. Collapse was imminent. Collapse is what the fuck I deserved.\n",
      "At the seventy-mile mark I couldn‚Äôt take another step forward. Kate had set\n",
      "up our lawn chair on the grass near the start/finish line and when I teetered\n",
      "toward her I saw her in triplicate, six hands groping toward me, guiding me\n",
      "into that folding chair. I was dizzy and dehydrated, starved of potassium\n",
      "and sodium.\n",
      "Kate was a nurse; I had EMT training, and went through my own mental\n",
      "checklist. I knew my blood pressure was probably dangerously low. She\n",
      "removed my shoes. My foot pain was no Shawn Dobbs illusion. My white\n",
      "\n",
      "---\n",
      "\n",
      "Oh yes, the pain was becoming real. My quads throbbed, my feet were\n",
      "chafed and bleeding, and that simple question was once again bubbling up\n",
      "in my frontal lobe. Why? Why run a hundred fucking miles without\n",
      "training? Why was I doing this to myself? Fair questions, especially since I\n",
      "hadn‚Äôt even heard of the San Diego One Day until three days before race\n",
      "day, but this time my answer was different. I wasn‚Äôt on Hospitality Point to\n",
      "deal with my own demons or to prove anything at all. I came with a purpose\n",
      "bigger than David Goggins. This fight was about my once and future fallen\n",
      "teammates, and the families they leave behind when shit goes wrong.\n",
      "Or at least that‚Äôs what I told myself at mile twenty-seven.\n",
      "* * *\n",
      "\n",
      "---\n",
      "\n",
      "Point would not make the cut. We‚Äôre talking about terrain so vanilla it‚Äôs\n",
      "downright serene. Tourists descend year-round for views of San Diego‚Äôs\n",
      "stunning marina, which spills into Mission Bay. The road is almost entirely\n",
      "smooth asphalt and perfectly flat, save a brief seven-foot incline with the\n",
      "pitch of a standard suburban driveway. There are manicured lawns, palm\n",
      "trees, and shade trees. Hospitality Point is so inviting that disabled and\n",
      "convalescing folks head there with their walkers for an afternoon‚Äôs rehab\n",
      "stroll, all the time. But the day after John Metz chalked his easy, one-mile\n",
      "course, it became the scene of my total destruction.\n",
      "I should have known that a breakdown was coming. By the time I started\n",
      "running at 10 a.m. on November 12, 2005, I hadn‚Äôt run more than a mile in\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: What medical problems did David Goggins suffer at mile 70 of the San Diego One Day race?\n",
      "\n",
      "\n",
      "Expected Answer:\n",
      "At mile 70, Goggins‚Äô body shut down due to a lack of training; he suffered from kidney failure, stress fractures, and lost control of his bladder and bowels while sitting in a lawn chair, yet he continued on to finish the race.\n",
      "\n",
      "Model Answer:\n",
      "Based on the text, it appears that \"David Goggins\" is being referenced as an example or inspiration to the author. However, since I don't have any information about the author's identity or their connection to David Goggins, I'll answer based on what is mentioned in the context.\n",
      "\n",
      "At mile 70 of the San Diego One Day race, it seems that the author (not David Goggins) was suffering from:\n",
      "\n",
      "* Dizziness and dehydration\n",
      "* Low blood pressure\n",
      "* Foot pain (specifically, their feet were chafed and bleeding)\n",
      "* A potassium and sodium deficiency\n",
      "\n",
      "These symptoms led to the author feeling like they were going to collapse.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[91mResponse: false.\u001b[0m\n",
      "False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[209]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_Cant_hurt_me2\u001b[39m():\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m query_and_validate(\n\u001b[32m      3\u001b[39m         \u001b[38;5;66;03m#question=\"What physical breakdown did David Goggins experience at mile 70 of the San Diego One Day race?\",\u001b[39;00m\n\u001b[32m      4\u001b[39m         question=\u001b[33m\"\u001b[39m\u001b[33mWhat medical problems did David Goggins suffer at mile 70 of the San Diego One Day race?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m         expected_response=\u001b[33m\"\u001b[39m\u001b[33mAt mile 70, Goggins‚Äô body shut down due to a lack of training; he suffered from kidney failure, stress fractures, and lost control of his bladder and bowels while sitting in a lawn chair, yet he continued on to finish the race.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mtest_Cant_hurt_me2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[209]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mtest_Cant_hurt_me2\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_Cant_hurt_me2\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m query_and_validate(\n\u001b[32m      3\u001b[39m         \u001b[38;5;66;03m#question=\"What physical breakdown did David Goggins experience at mile 70 of the San Diego One Day race?\",\u001b[39;00m\n\u001b[32m      4\u001b[39m         question=\u001b[33m\"\u001b[39m\u001b[33mWhat medical problems did David Goggins suffer at mile 70 of the San Diego One Day race?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m         expected_response=\u001b[33m\"\u001b[39m\u001b[33mAt mile 70, Goggins‚Äô body shut down due to a lack of training; he suffered from kidney failure, stress fractures, and lost control of his bladder and bowels while sitting in a lawn chair, yet he continued on to finish the race.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def test_Cant_hurt_me2():\n",
    "    assert query_and_validate(\n",
    "        #question=\"What physical breakdown did David Goggins experience at mile 70 of the San Diego One Day race?\",\n",
    "        question=\"What medical problems did David Goggins suffer at mile 70 of the San Diego One Day race?\",\n",
    "        expected_response=\"At mile 70, Goggins‚Äô body shut down due to a lack of training; he suffered from kidney failure, stress fractures, and lost control of his bladder and bowels while sitting in a lawn chair, yet he continued on to finish the race.\",\n",
    "    )\n",
    "test_Cant_hurt_me2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c65ded12-bd93-4bdf-8f7f-b4bbfc5d13c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8f78c250a3436dbae14952cc1e9cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "accepting Trunnis Goggins as part of me, I was free to use where I came\n",
      "from as fuel. I realized that each episode of child abuse that could have\n",
      "killed me made me tough as hell and as sharp as a Samurai‚Äôs blade.\n",
      "True, I had been dealt a fucked-up hand, but that night I started thinking of it\n",
      "as running a 100-mile race with a fifty-pound ruck on my back. Could I still\n",
      "compete in that race even if everyone else was running free and easy,\n",
      "weighing 130 pounds? How fast would I be able to run once I‚Äôd shed that\n",
      "dead weight? I wasn‚Äôt even thinking about ultras yet. To me the race was life\n",
      "itself, and the more I took inventory, the more I realized how prepared I was\n",
      "for the fucked-up events yet to come. Life had put me in the fire, taken me\n",
      "\n",
      "---\n",
      "\n",
      "typhoon.\n",
      "‚ÄúPeople have a hard time going through BUD/S healthy, and you‚Äôre going\n",
      "through it on broken legs! Who else would even think of this?‚Äù I asked.\n",
      "‚ÄúWho else would be able to run even one minute on one broken leg, let alone\n",
      "two? Only Goggins! You are twenty minutes in the business, Goggins! You\n",
      "are a fucking machine! Each step you run from now until the end will only\n",
      "make you harder!‚Äù\n",
      "That last message cracked the code like a password. My calloused mind was\n",
      "my ticket forward, and at the forty-minute mark something remarkable\n",
      "happened. The pain receded to low tide. The tape had loosened so it wasn‚Äôt\n",
      "cutting into my skin, and my muscles and bones were warm enough to take\n",
      "some pounding. The pain would come and go throughout the day, but it\n",
      "\n",
      "---\n",
      "\n",
      "Oh yes, the pain was becoming real. My quads throbbed, my feet were\n",
      "chafed and bleeding, and that simple question was once again bubbling up\n",
      "in my frontal lobe. Why? Why run a hundred fucking miles without\n",
      "training? Why was I doing this to myself? Fair questions, especially since I\n",
      "hadn‚Äôt even heard of the San Diego One Day until three days before race\n",
      "day, but this time my answer was different. I wasn‚Äôt on Hospitality Point to\n",
      "deal with my own demons or to prove anything at all. I came with a purpose\n",
      "bigger than David Goggins. This fight was about my once and future fallen\n",
      "teammates, and the families they leave behind when shit goes wrong.\n",
      "Or at least that‚Äôs what I told myself at mile twenty-seven.\n",
      "* * *\n",
      "\n",
      "---\n",
      "\n",
      "by a Zodiac and motored to shore just four minutes before the gun. That was\n",
      "barely enough time for a blast of energy gel, a swig of water, and to change\n",
      "into our Navy SEAL triathlon suits.\n",
      "You know by now that I‚Äôm slow in the water, and Davids destroyed my ass\n",
      "on the 2.4-mile swim. I‚Äôm just as strong as he is on a bicycle, but my lower\n",
      "back tightened up that day and at the halfway point I had to stop and stretch\n",
      "out. By the time I coasted into the transition area after a 112-mile bike ride,\n",
      "Davids had thirty minutes on me, and early on in the marathon, I didn‚Äôt do a\n",
      "great job of getting any of it back. My body was rebelling and I had to walk\n",
      "those early miles, but I stayed in the fight, and at mile ten found a rhythm\n",
      "and started clipping time. Somewhere ahead of me Davids blew up, and I\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: According to Can't Hurt Me, what humiliating physical incident did David Goggins admit happened to him during the San Diego One Day race after his body began to fail?\n",
      "\n",
      "\n",
      "Expected Answer:\n",
      "He admitted that he lost control of his bowels during the race but kept going anyway.\n",
      "\n",
      "Model Answer:\n",
      "According to the text, at mile ten of the marathon, David Goggins had to walk due to his body rebelling, but he stayed in the fight and found a rhythm to start clipping time. However, there is no mention of a specific humiliating physical incident during this part of the race.\n",
      "\n",
      "It seems I made an incorrect assumption about the question. Since the text does not explicitly state what you're asking for (a \"humiliating physical incident\"), I would say that I don't have enough information to provide a correct answer based on the provided context.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[91mResponse: false.\u001b[0m\n",
      "False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[212]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_Cant_hurt_me3\u001b[39m():\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m query_and_validate(\n\u001b[32m      3\u001b[39m     question = \u001b[33m\"\u001b[39m\u001b[33mAccording to Can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt Hurt Me, what humiliating physical incident did David Goggins admit happened to him during the San Diego One Day race after his body began to fail?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     expected_response = \u001b[33m\"\u001b[39m\u001b[33mHe admitted that he lost control of his bowels during the race but kept going anyway.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     )\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtest_Cant_hurt_me3\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[212]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mtest_Cant_hurt_me3\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_Cant_hurt_me3\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m query_and_validate(\n\u001b[32m      3\u001b[39m     question = \u001b[33m\"\u001b[39m\u001b[33mAccording to Can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt Hurt Me, what humiliating physical incident did David Goggins admit happened to him during the San Diego One Day race after his body began to fail?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     expected_response = \u001b[33m\"\u001b[39m\u001b[33mHe admitted that he lost control of his bowels during the race but kept going anyway.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def test_Cant_hurt_me3():\n",
    "    assert query_and_validate(\n",
    "    question = \"According to Can't Hurt Me, what humiliating physical incident did David Goggins admit happened to him during the San Diego One Day race after his body began to fail?\",\n",
    "    expected_response = \"He admitted that he lost control of his bowels during the race but kept going anyway.\",\n",
    "    )\n",
    "test_Cant_hurt_me3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c396ec-2451-44db-af94-489d4fe15915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a90fe5f4-924c-4341-a924-55274d98cef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_RCNN_Paper():\n",
    "    assert query_and_validate(\n",
    "        question = 'What dataset was used?',\n",
    "        expected_response = 'PASCAL VOC',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1649ab1d-3571-49d0-aa00-aef9c4c40051",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc29d906c724e3f933ce73ae2310344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "PASCAL VOC\n",
      "\n",
      "Model Answer:\n",
      "The datasets used are PASCAL VOC 2007 and PASCAL VOC 2012. Specifically:\n",
      "\n",
      "* Table 1 uses PASCAL VOC (no specific year mentioned)\n",
      "* Table 2 reports results for PASCAL VOC 2007 test set\n",
      "* Table 3 reports results for PASCAL VOC 2012 test set\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[92mResponse: true\u001b[0m\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "test_RCNN_Paper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070384e-4887-41b3-ad4c-14f6ebc9e71c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Questions sets to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "55419085-a74e-4dfe-95f9-a58e91a3bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANT_HURT_ME_TESTS = [\n",
    "    {\n",
    "        \"question\": \"How many hell weeks did Goggins do?\",\n",
    "        \"expected\": \"He did three Hell Weeks\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What was David Goggins' max weight?\",\n",
    "        \"expected\": \"297 pounds\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Before losing weight and training, which military unit was Goggins trying to join?\",\n",
    "        \"expected\": \"The Navy SEALs\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"After serving as a SEAL, which elite Army unit did Goggins consider attempting to join?\",\n",
    "        \"expected\": \"Delta Force\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How many times did Goggins fail his pull-up record attempt?\",\n",
    "        \"expected\": \"He failed twice\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What was the previous pull-up world record?\",\n",
    "        \"expected\": \"4000 pull-ups\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What pull-up record did Goggins set?\",\n",
    "        \"expected\": \"4030 pull-ups\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What races did Goggins run before Badwater 135?\",\n",
    "        \"expected\": \"San Diego One Day and Hurt 100\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"At what mile in the San Diego One Day race did Goggins soil himself?\",\n",
    "        \"expected\": \"Mile 70\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "78c9557b-70ca-4a0e-a793-8a54a99c0466",
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTER_RCNN_TESTS = [\n",
    "    {\n",
    "        \"question\": \"What is a RPN?\",\n",
    "        \"expected\": \"A fully convolutional network that predicts object bounding boxes and objectness scores from shared feature maps to generate region proposals\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What dataset was used?\",\n",
    "        \"expected\": \"PASCAL VOC\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What task does Faster R-CNN perform?\",\n",
    "        \"expected\": \"Object detection\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What does Faster R-CNN improve over earlier R-CNN variants?\",\n",
    "        \"expected\": \"It replaces external region proposal methods with a learned Region Proposal Network for end-to-end training\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the role of anchors in Faster R-CNN?\",\n",
    "        \"expected\": \"They are predefined boxes of different scales and aspect ratios used to propose candidate object regions\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What two outputs does the RPN predict for each anchor?\",\n",
    "        \"expected\": \"Bounding box offsets and objectness scores\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What backbone network is commonly used in Faster R-CNN?\",\n",
    "        \"expected\": \"A convolutional neural network such as VGG or ResNet\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is ROI pooling used for?\",\n",
    "        \"expected\": \"To convert variable-sized region proposals into fixed-size feature maps\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What loss components are used to train Faster R-CNN?\",\n",
    "        \"expected\": \"Classification loss and bounding box regression loss\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What does non-maximum suppression do in Faster R-CNN?\",\n",
    "        \"expected\": \"It removes highly overlapping bounding boxes, keeping only the highest scoring ones\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the purpose of sharing convolutional features between the RPN and detection network?\",\n",
    "        \"expected\": \"To reduce computation and enable joint optimization\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How are positive anchors defined during training?\",\n",
    "        \"expected\": \"Anchors with high intersection-over-union overlap with a ground-truth box\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the main output of Faster R-CNN at inference time?\",\n",
    "        \"expected\": \"Class labels and refined bounding boxes for detected objects\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "49a26a4b-da54-4da2-b48a-8d9ca312046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_set(test_set_name: str):\n",
    "    if test_set_name == \"cant_hurt_me\":\n",
    "        tests = CANT_HURT_ME_TESTS\n",
    "    elif test_set_name == \"faster_rcnn\":\n",
    "        tests = FASTER_RCNN_TESTS\n",
    "    else:\n",
    "        raise ValueError(\"Unknown test set. Use 'cant_hurt_me' or 'faster_rcnn'.\")\n",
    "\n",
    "    print(f\"\\nRunning tests for: {test_set_name}\\n\")\n",
    "\n",
    "    passed = 0\n",
    "\n",
    "    for i, t in enumerate(tests, 1):\n",
    "        print(f\"Test {i}: {t['question']}\")\n",
    "        ok = query_and_validate(\n",
    "            question=t[\"question\"],\n",
    "            expected_response=t[\"expected\"],\n",
    "        )\n",
    "\n",
    "        if ok:\n",
    "            print(\"PASS\\n\")\n",
    "            passed += 1\n",
    "        else:\n",
    "            print(\"FAIL\\n\")\n",
    "\n",
    "    print(f\"Summary: {passed}/{len(tests)} passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4a35d31f-2c2a-4d4f-9b91-dc1b66f69281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running tests for: cant_hurt_me\n",
      "\n",
      "Test 1: How many hell weeks did Goggins do?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4f2b5a9b9b4db195b5d0f8e0670c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "He did three Hell Weeks\n",
      "\n",
      "Model Answer:\n",
      "Based on the context, Goggins has done at least 2 Hell Weeks, as mentioned in the sentence:\n",
      "\n",
      "\"After surviving two Hell Weeks and participating in three...\"\n",
      "\n",
      "So, he has either participated or survived a total of 5 Hell Weeks.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[92mResponse: true\u001b[0m\n",
      "True\n",
      "PASS\n",
      "\n",
      "Test 2: What was David Goggins' max weight?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f4838f58ac4c508d94a9f7993a4f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "297 pounds\n",
      "\n",
      "Model Answer:\n",
      "According to the text, David Goggins weighed:\n",
      "\n",
      "* 255 pounds in his last days in the Air Force\n",
      "* Nearly 300 pounds after he continued to bulk up after his discharge.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[92mResponse: true.\u001b[0m\n",
      "True\n",
      "PASS\n",
      "\n",
      "Test 3: Before losing weight and training, which military unit was Goggins trying to join?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214c1ecdd7af4d42882d1dee4c421d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "The Navy SEALs\n",
      "\n",
      "Model Answer:\n",
      "Before losing weight and training, David Goggins was trying to join DEVGRU (a Navy SEAL unit), specifically Green Team, their training program. He had been approved by SEAL Team Five brass to screen for Green Team, but he had yet to attend Army Ranger School before doing so.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[91mResponse: false. \n",
      "\n",
      "the model answer refers to \"devgru (a navy seal unit)\" and specifically mentions \"green team\", whereas the expected answer only mentions \"the navy seals\". the entities differ in scope and specificity.\u001b[0m\n",
      "False\n",
      "FAIL\n",
      "\n",
      "Test 4: After serving as a SEAL, which elite Army unit did Goggins consider attempting to join?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c56fd6981344626ae3c97b37d9bda02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "Delta Force\n",
      "\n",
      "Model Answer:\n",
      "Delta Force\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[92mResponse: true\u001b[0m\n",
      "True\n",
      "PASS\n",
      "\n",
      "Test 5: How many times did Goggins fail his pull-up record attempt?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3097fcba5ca419d9343eafd128e81cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "He failed twice\n",
      "\n",
      "Model Answer:\n",
      "According to the text, Goggins failed his pull-up record attempt twice before finally breaking the record with 4,030 pull-ups in 17 hours.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[92mResponse: true\u001b[0m\n",
      "True\n",
      "PASS\n",
      "\n",
      "Test 6: What was the previous pull-up world record?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3bf23573e34cb8a22d1bf2aef24179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "4000 pull-ups\n",
      "\n",
      "Model Answer:\n",
      "According to the text, the author's goal was to break Stephen Hyland's record of 4,020 pull-ups in a twenty-four hour period, but there is no information provided about what the previous record was before that. However, it does mention that \"after my second failure\" and notes that the author was still over 800 pull-ups away from the target of 4,020, suggesting that Stephen Hyland's record may have been set previously or was a known standard at the time of the author's attempt.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[92mResponse: true.\u001b[0m\n",
      "True\n",
      "PASS\n",
      "\n",
      "Test 7: What pull-up record did Goggins set?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41bfabfe1f784bc7b852546b298fa956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "4030 pull-ups\n",
      "\n",
      "Model Answer:\n",
      "Goggins set the 24-hour pull-up record of 4,030.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[92mResponse: true.\u001b[0m\n",
      "True\n",
      "PASS\n",
      "\n",
      "Test 8: What races did Goggins run before Badwater 135?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550afc9cae034886b0ae24951577cd84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "San Diego One Day and Hurt 100\n",
      "\n",
      "Model Answer:\n",
      "Unfortunately, the text doesn't explicitly state what specific races Goggins ran before Badwater 135. However, it does mention that he had previously watched Scott Jurek win the 2006 edition of Badwater and that he was inspired to raise money for the Special Operations Warrior Foundation by doing an endurance event, which ultimately led him to decide to run Badwater 135.\n",
      "\n",
      "The text also mentions \"Hell Week\" as a reference point when discussing Goggins' reaction to seeing images from Badwater. Given the context of the story and Hell Week's notorious reputation in Navy SEAL training, it can be inferred that Goggins was likely involved in military or law enforcement training before attempting Badwater 135.\n",
      "\n",
      "Additionally, earlier in his career, he mentions running marathons, which were previously considered the pinnacle of endurance racing.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[91mResponse: false.\u001b[0m\n",
      "False\n",
      "FAIL\n",
      "\n",
      "Test 9: At what mile in the San Diego One Day race did Goggins soil himself?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b3f267da12409e809d2957dbc0b710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "Mile 70\n",
      "\n",
      "Model Answer:\n",
      "There is no mention of Goggins soiling himself at any point in the text. The narrative does describe Goggins' physical suffering and his severe dehydration, but it does not include an incident where he soils himself.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[91mResponse: false\u001b[0m\n",
      "False\n",
      "FAIL\n",
      "\n",
      "Summary: 6/9 passed.\n"
     ]
    }
   ],
   "source": [
    "run_test_set(\"cant_hurt_me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2f28db15-691f-4f2a-b2b0-6e2642c7b7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running tests for: faster_rcnn\n",
      "\n",
      "Test 1: What is a RPN?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b900a00752f42ae8e74b072d9f8a4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "A fully convolutional network that predicts object bounding boxes and objectness scores from shared feature maps to generate region proposals\n",
      "\n",
      "Model Answer:\n",
      "A Region Proposal Network (RPN) is a fully-convolutional network that simultaneously predicts object bounds and objectness scores for potential objects in an image.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[92mResponse: true.\u001b[0m\n",
      "True\n",
      "PASS\n",
      "\n",
      "Test 2: What dataset was used?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557cb1b7149d4e7590bf51ad2a32987b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "PASCAL VOC\n",
      "\n",
      "Model Answer:\n",
      "The datasets mentioned are:\n",
      "\n",
      "1. PASCAL VOC 2007 test set\n",
      "2. VOC 2012 trainval\n",
      "3. VOC 2007 trainval\n",
      "4. Union set of VOC 2007 trainval and VOC 2012 trainval (denoted as \"07+12\" or \"07++12\")\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[91mResponse: false.\u001b[0m\n",
      "False\n",
      "FAIL\n",
      "\n",
      "Test 3: What task does Faster R-CNN perform?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288ff7939a954af8ba28be79a558be0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "Object detection\n",
      "\n",
      "Model Answer:\n",
      "Faster R-CNN performs object detection, including:\n",
      "\n",
      "* Hypothesizing object locations (using a Region Proposal Network, or RPN)\n",
      "* Predicting class-specific scores and regressing box locations for detected objects.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[92mResponse: true.\u001b[0m\n",
      "True\n",
      "PASS\n",
      "\n",
      "Test 4: What does Faster R-CNN improve over earlier R-CNN variants?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5186ca97ac64dd7a546beb3cc011564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "It replaces external region proposal methods with a learned Region Proposal Network for end-to-end training\n",
      "\n",
      "Model Answer:\n",
      "Based on the provided context, Faster R-CNN (which uses a Region Proposal Network, or RPN) improves over earlier R-CNN variants by providing \"nearly cost-free region proposals\" and achieving better accuracy. Specifically, it is mentioned that Faster R-CNN with an RPN has an mAP of 73.2%, which is higher than the 68.4% achieved by SS on the union set of VOC 2007 trainval+test and VOC 2012 trainval. Additionally, Faster R-CNN reduces the running time of object detection systems compared to earlier methods that use sliding windows.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[92mResponse: true\u001b[0m\n",
      "True\n",
      "PASS\n",
      "\n",
      "Test 5: What is the role of anchors in Faster R-CNN?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073c1e5034a2424b89a7e45c5ab1727a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "They are predefined boxes of different scales and aspect ratios used to propose candidate object regions\n",
      "\n",
      "Model Answer:\n",
      "Actually, the context does not explicitly mention \"Faster R-CNN\", but it does talk about Region Proposal Networks (RPN) and Fast R-CNN. According to the text, the number of anchor locations is mentioned as being approximately 2,400 (i.e., Nreg ‚àº 2,400).\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[91mResponse: false\u001b[0m\n",
      "False\n",
      "FAIL\n",
      "\n",
      "Test 6: What two outputs does the RPN predict for each anchor?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431073b63f3b4e30a15cf77f32a24a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "Bounding box offsets and objectness scores\n",
      "\n",
      "Model Answer:\n",
      "The RPN predicts two outputs for each anchor:\n",
      "\n",
      "1. A binary class label (object vs not object) represented by pi, where pi is the predicted probability of anchor i being an object.\n",
      "2. The 4 parameterized coordinates of the predicted bounding box ti, which represents the location and size of the proposed bounding box.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[92mResponse: true\u001b[0m\n",
      "True\n",
      "PASS\n",
      "\n",
      "Test 7: What backbone network is commonly used in Faster R-CNN?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b44756f02b42f19a2ae9aa7874160f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "A convolutional neural network such as VGG or ResNet\n",
      "\n",
      "Model Answer:\n",
      "The text does not explicitly mention that a specific backbone network is commonly used in Faster R-CNN. However, it mentions \"ZF\" and \"VGG nets\" as networks that are tested with single-scale feature extraction.\n",
      "\n",
      "Upon further review of the paper's abstract, it appears to imply that the VGG-16 model (a variant of VGGNet) is being used for object detection in the Faster R-CNN system.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[92mResponse: true\u001b[0m\n",
      "True\n",
      "PASS\n",
      "\n",
      "Test 8: What is ROI pooling used for?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e9dbc49f1e4f2b90f26afae6f607cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "To convert variable-sized region proposals into fixed-size feature maps\n",
      "\n",
      "Model Answer:\n",
      "The provided text does not explicitly mention \"ROI pooling\" by name, but it does discuss a similar concept called \"Spatial Pyramid Pooling (SPP)\" which is used in deep convolutional networks for visual recognition. However, since Fast R-CNN and Region Proposal Networks are mentioned, it can be inferred that ROI (Region of Interest) pooling is related to the extraction of features from objects or regions.\n",
      "\n",
      "ROI pooling is a technique used to extract fixed-size feature maps from variable-sized regions or objects in an image, allowing for the use of shared convolutions across different sized inputs. This is relevant to object detection and classification tasks.\n",
      "\n",
      "Given the context, it can be assumed that ROI pooling is used for efficient region-based object detection by extracting features from regions of interest in the image.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[92mResponse: true\u001b[0m\n",
      "True\n",
      "PASS\n",
      "\n",
      "Test 9: What loss components are used to train Faster R-CNN?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5ca1d685e04efcad1f0b69f38f4ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "Classification loss and bounding box regression loss\n",
      "\n",
      "Model Answer:\n",
      "The text does not explicitly mention \"Faster R-CNN\" but it mentions training both region proposal and object detection networks. According to the context, these networks use two types of loss terms:\n",
      "\n",
      "1. cls (classification) term\n",
      "2. reg (regression) term\n",
      "\n",
      "These terms are roughly equally weighted during training.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[92mResponse: true\u001b[0m\n",
      "True\n",
      "PASS\n",
      "\n",
      "Test 10: What does non-maximum suppression do in Faster R-CNN?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c825316006c4d828c0de21c3c61f3ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "It removes highly overlapping bounding boxes, keeping only the highest scoring ones\n",
      "\n",
      "Model Answer:\n",
      "Non-maximum suppression (NMS) reduces redundancy by eliminating proposal regions that have a high overlap with other proposal regions, leaving only the top-ranked proposal regions for detection. In this specific implementation, NMS uses an IoU threshold of 0.7 and leaves about 2k proposal regions per image.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[92mResponse: true.\u001b[0m\n",
      "True\n",
      "PASS\n",
      "\n",
      "Test 11: What is the purpose of sharing convolutional features between the RPN and detection network?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b5aff658e44650884833549955c96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "To reduce computation and enable joint optimization\n",
      "\n",
      "Model Answer:\n",
      "The purpose of sharing convolutional features between the Region Proposal Network (RPN) and detection network is to improve the performance of both networks by allowing them to learn from each other's features. Specifically, it is mentioned that when the two networks share conv layers, \"the proposal quality is improved\" in the third step of the 4-step training process. This suggests that sharing features enables the RPN to generate better proposals for the detection network, which in turn improves the detection performance.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[92mResponse: true\u001b[0m\n",
      "True\n",
      "PASS\n",
      "\n",
      "Test 12: How are positive anchors defined during training?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da47ea0baae47598a250f00b94cb2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "Anchors with high intersection-over-union overlap with a ground-truth box\n",
      "\n",
      "Model Answer:\n",
      "During training, a positive anchor is defined as either:\n",
      "\n",
      "(i) an anchor that has an IoU overlap higher than 0.7 with any ground-truth box, or\n",
      "(ii) an anchor that has the highest Intersection-Over-Union (IoU) overlap with a ground-truth box.\n",
      "\n",
      "In other words, anchors are assigned a positive label if they have a high enough IoU overlap with any of the ground-truth boxes in the image.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[92mResponse: true.\u001b[0m\n",
      "True\n",
      "PASS\n",
      "\n",
      "Test 13: What is the main output of Faster R-CNN at inference time?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6952d7de5718438e849812c2929e6fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer:\n",
      "Class labels and refined bounding boxes for detected objects\n",
      "\n",
      "Model Answer:\n",
      "According to Table 4, the \"Region-wise\" includes NMS, pooling, fc, and softmax. This suggests that the main output of Faster R-CNN at inference time is the class scores (output of softmax) and bounding box predictions (output of NMS and pooling), which are then combined with features extracted from a convolutional neural network to generate the final object detection results.\n",
      "\n",
      "---\n",
      "\n",
      "Determine whether the Model Answer states the same factual claim(s) as the Expected Answer.\n",
      "\n",
      "Rules:\n",
      "- If any number, count, or entity differs, answer false.\n",
      "- If the Expected Answer is contradicted, answer false.\n",
      "- Do NOT be generous.\n",
      "- Do NOT infer or reinterpret.\n",
      "- Ignore wording only when the facts clearly match.\n",
      "\n",
      "Respond with exactly one word: true or false.\n",
      "\n",
      "\u001b[91mResponse: false\u001b[0m\n",
      "False\n",
      "FAIL\n",
      "\n",
      "Summary: 10/13 passed.\n"
     ]
    }
   ],
   "source": [
    "run_test_set(\"faster_rcnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8a7d04-cdbf-4a44-b8f3-dcb4838e2822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a976aa0-e640-43c3-bedc-14e10ef6a8ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Adding chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "74cab0f1-a063-4280-aa53-255282b8e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_HISTORY = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "49409541-9580-4ca5-827e-bc78ba199e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_chat_history():\n",
    "    CHAT_HISTORY.clear()\n",
    "    print(\"Chat history reset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b85a6f44-3a68-40fa-806f-4f6176e2c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWRITE_PROMPT = \"\"\"\n",
    "Given the chat history and the latest question, rewrite the question so it is standalone and can be understood without the history.\n",
    "\n",
    "Chat history:\n",
    "{history}\n",
    "\n",
    "Latest question: {question}\n",
    "\n",
    "Standalone question:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "391e9686-17b6-4d7a-a56a-9a5f0a0b1ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query_with_history(query: str, history: list):\n",
    "    if not history:\n",
    "        return query\n",
    "\n",
    "    model = OllamaLLM(model=\"llama3.1\")\n",
    "\n",
    "    history_text = \"\\n\".join(f\"{role}: {msg}\" for role, msg in history[-6:])\n",
    "\n",
    "    prompt = REWRITE_PROMPT.format(history=history_text,question=query,)\n",
    "    #print(prompt)\n",
    "    \n",
    "    rewritten = model.invoke(prompt)\n",
    "    return rewritten.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1d5b771d-cb6c-484a-870e-5583426df049",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base_PROMPT_2  = \"\"\"\n",
    "Chat history:\n",
    "{history}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer the question based on the above context\n",
    "\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a4368-961f-4153-aef7-5efd605dac70",
   "metadata": {},
   "source": [
    "query_rag(query_text, history)  \n",
    "    ‚Üí rewrite question using history  \n",
    "    ‚Üí embed rewritten query  \n",
    "    ‚Üí similarity_search  \n",
    "    ‚Üí build prompt with context + history  \n",
    "    ‚Üí LLM  \n",
    "    ‚Üí update history  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846f4f59-a7e1-4991-90ee-2c39103abe85",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Old but working function with no reranker only top k=4 similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "07885e3f-c17d-4173-a88d-5aae0193a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def query_rag_hist(query_text: str, history: list):\n",
    "#     emb_fxn = get_embeddings_function()\n",
    "#     db = Chroma(persist_directory=CHROMA_DIR,embedding_function = emb_fxn)\n",
    "\n",
    "#     standalone_query = rewrite_query_with_history(query_text, history)\n",
    "    \n",
    "#     results = db.similarity_search_with_score(standalone_query, k=4)\n",
    "\n",
    "#     context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "\n",
    "#     history_text = \"\\n\".join(f\"{role}: {msg}\" for role, msg in history[-6:])\n",
    "    \n",
    "#     prompt_template = ChatPromptTemplate.from_template(Base_PROMPT_2\n",
    "#                                                       )\n",
    "#     prompt = prompt_template.format(context=context, question=query_text, history=history_text,)\n",
    "#     #print(prompt)\n",
    "\n",
    "#     model = OllamaLLM(model = 'llama3.1')\n",
    "#     response_text = model.invoke(prompt)\n",
    "\n",
    "#     history.append((\"user\", query_text))\n",
    "#     history.append((\"assistant\", response_text))\n",
    "\n",
    "#     sources = [doc.metadata.get('id', None) for doc, _source in results]\n",
    "    \n",
    "#     print(\"Standalone query:\", standalone_query)\n",
    "    \n",
    "#     print(\"\\nResponse: \",response_text)\n",
    "#     print(\"\\nSources:\", sources)\n",
    "\n",
    "#     return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7424a3-e3ad-45ef-a286-c24c2e3eb184",
   "metadata": {},
   "source": [
    "### New Fxn using reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "b98c4326-e9c6-40f1-944b-bf05ea897e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag_hist(query_text: str, history: list, return_context=False, return_sources=False):\n",
    "    standalone_query = rewrite_query_with_history(query_text, history)\n",
    "    \n",
    "    results = db.similarity_search_with_score(standalone_query, k=10)\n",
    "    reranked_docs = rerank(standalone_query, results)\n",
    "\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in reranked_docs])\n",
    "    \n",
    "    history_text = \"\\n\".join(f\"{role}: {msg}\" for role, msg in history[-6:])\n",
    "    prompt_template = ChatPromptTemplate.from_template(Base_PROMPT_2)\n",
    "    prompt = prompt_template.format(context=context, question=query_text, history=history_text,)\n",
    "    #print(prompt)\n",
    "\n",
    "    response_text = model.invoke(prompt)\n",
    "    history.append((\"user\", query_text))\n",
    "    history.append((\"assistant\", response_text))\n",
    "    sources = [doc.metadata.get('id', None) for doc in reranked_docs]\n",
    "\n",
    "    #print(\"Standalone query:\", standalone_query)\n",
    "    #print(\"\\nResponse: \",response_text)\n",
    "    #print(\"\\nSources:\", sources)\n",
    "    if return_context and return_sources:\n",
    "        return response_text, context, sources\n",
    "    \n",
    "    if return_sources:\n",
    "        return response_text, sources\n",
    "    \n",
    "    if return_context:\n",
    "        return response_text, context\n",
    "    \n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee0e457-ded4-4e9f-9e31-636862e87dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "42b251e2-6595-44be-9d8e-cfc9ee338f2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standalone query: What is an RPN?\n",
      "\n",
      "Response:  An RPN (Region Proposal Network) is a fully-convolutional network that simultaneously predicts object bounds and ratios at a location. It shares full-image convolutional features with the detection network, allowing for nearly cost-free region proposals.\n",
      "\n",
      "Sources: ['C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Faster-RCNN-Paper.pdf:0:0', 'C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Faster-RCNN-Paper.pdf:1:1', 'C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Faster-RCNN-Paper.pdf:3:3', 'C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Faster-RCNN-Paper.pdf:5:3']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'An RPN (Region Proposal Network) is a fully-convolutional network that simultaneously predicts object bounds and ratios at a location. It shares full-image convolutional features with the detection network, allowing for nearly cost-free region proposals.'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rag_hist(\"What is an RPN?\", CHAT_HISTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c88247f5-d874-41d4-abb3-601f93d0a20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user', 'What is an RPN?'),\n",
       " ('assistant',\n",
       "  'An RPN (Region Proposal Network) is a fully-convolutional network that simultaneously predicts object bounds and ratios at a location. It shares full-image convolutional features with the detection network, allowing for nearly cost-free region proposals.')]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAT_HISTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b64b89f4-1cf3-42be-ab51-9a2dfd1e188e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standalone query: How does an RPN (Region Proposal Network) differ from Fast R-CNN?\n",
      "\n",
      "Response:  Based on the provided context, it appears that the Region Proposal Network (RPN) is different from Fast R-CNN in several ways:\n",
      "\n",
      "1. **Purpose**: The primary purpose of RPN is to generate region proposals, whereas Fast R-CNN uses these proposals for object detection.\n",
      "2. **Shared features**: RPN shares full-image convolutional features with the detection network, enabling nearly cost-free region proposals, while Fast R-CNN relies on separate region proposal algorithms.\n",
      "3. **Training approach**: The training process involves alternating optimization between the RPN and Fast R-CNN to learn shared convolutional features.\n",
      "\n",
      "However, it is worth noting that both RPN and Fast R-CNN can be combined using a 4-step training algorithm (alternating optimization) to share conv layers, enabling state-of-the-art object detection accuracy with fast inference.\n",
      "\n",
      "Sources: ['C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Faster-RCNN-Paper.pdf:0:0', 'C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Faster-RCNN-Paper.pdf:0:1', 'C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Faster-RCNN-Paper.pdf:3:5', 'C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Faster-RCNN-Paper.pdf:4:0']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, it appears that the Region Proposal Network (RPN) is different from Fast R-CNN in several ways:\\n\\n1. **Purpose**: The primary purpose of RPN is to generate region proposals, whereas Fast R-CNN uses these proposals for object detection.\\n2. **Shared features**: RPN shares full-image convolutional features with the detection network, enabling nearly cost-free region proposals, while Fast R-CNN relies on separate region proposal algorithms.\\n3. **Training approach**: The training process involves alternating optimization between the RPN and Fast R-CNN to learn shared convolutional features.\\n\\nHowever, it is worth noting that both RPN and Fast R-CNN can be combined using a 4-step training algorithm (alternating optimization) to share conv layers, enabling state-of-the-art object detection accuracy with fast inference.'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rag_hist(\"How does it differ from Fast R-CNN?\", CHAT_HISTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ca973dda-d682-4c44-90f6-bc59c9920fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user', 'What is an RPN?'),\n",
       " ('assistant',\n",
       "  'An RPN (Region Proposal Network) is a fully-convolutional network that simultaneously predicts object bounds and ratios at a location. It shares full-image convolutional features with the detection network, allowing for nearly cost-free region proposals.'),\n",
       " ('user', 'How does it differ from Fast R-CNN?'),\n",
       " ('assistant',\n",
       "  'Based on the provided context, it appears that the Region Proposal Network (RPN) is different from Fast R-CNN in several ways:\\n\\n1. **Purpose**: The primary purpose of RPN is to generate region proposals, whereas Fast R-CNN uses these proposals for object detection.\\n2. **Shared features**: RPN shares full-image convolutional features with the detection network, enabling nearly cost-free region proposals, while Fast R-CNN relies on separate region proposal algorithms.\\n3. **Training approach**: The training process involves alternating optimization between the RPN and Fast R-CNN to learn shared convolutional features.\\n\\nHowever, it is worth noting that both RPN and Fast R-CNN can be combined using a 4-step training algorithm (alternating optimization) to share conv layers, enabling state-of-the-art object detection accuracy with fast inference.')]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAT_HISTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7463fc-0a8a-4096-ad42-16e9ef02c47e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a78d2c12-8ed0-464e-b8f5-eabfe08da983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history reset.\n"
     ]
    }
   ],
   "source": [
    "reset_chat_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "32d37af6-5288-4eb7-9f15-65a6fd67b5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAT_HISTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ffdcf9-4a1d-47f6-a946-ab9d3a125deb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Evaluation Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c134d3ae-c297-426b-afe5-d6aee3e389fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da00a8d0199f4daba6903260948cd1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "emb = get_embeddings_function()\n",
    "db = Chroma(persist_directory=str(CHROMA_DIR), embedding_function=emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0cdb1d-0f21-4318-b3d4-ab7faeed9083",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### This part was run 30 times to make the json files with the question , expected, gold pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "78ce4bfb-037d-4261-a04f-aae1afb4db4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What phrase does Goggins use to describe the mental callous formed through suffering?\n",
      "floods our soul, and influences the decisions which determine our character.\n",
      "My fears were never just about the water, and my anxieties toward Class 235\n",
      "weren‚Äôt about the pain of First Phase. They were seeping from the infected\n",
      "wounds I‚Äôd been walking around with my entire life, and my denial of them\n",
      "amounted to a denial of myself. I was my own worst enemy! It wasn‚Äôt the\n",
      "world, or God, or the Devi\n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Data\\Can_t-Hurt-Me-David-Goggins.pdf:120:1\n",
      "------------------------------------------------------------------------------------------\n",
      "What phrase does Goggins use to describe the mental callous formed through suffering?\n",
      "pool. I didn‚Äôt want to say anything because I didn‚Äôt yet understand what I\n",
      "now know.\n",
      "Similar to using an opponent‚Äôs energy to gain an advantage, leaning on your\n",
      "calloused mind in the heat of battle can shift your thinking as well.\n",
      "Remembering what you‚Äôve been through and how that has strengthened your\n",
      "mindset can lift you out of a negative brain loop and help you bypass those\n",
      "weak, one-second impu\n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Data\\Can_t-Hurt-Me-David-Goggins.pdf:114:0\n",
      "------------------------------------------------------------------------------------------\n",
      "What phrase does Goggins use to describe the mental callous formed through suffering?\n",
      "accepting Trunnis Goggins as part of me, I was free to use where I came\n",
      "from as fuel. I realized that each episode of child abuse that could have\n",
      "killed me made me tough as hell and as sharp as a Samurai‚Äôs blade.\n",
      "True, I had been dealt a fucked-up hand, but that night I started thinking of it\n",
      "as running a 100-mile race with a fifty-pound ruck on my back. Could I still\n",
      "compete in that race even if \n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Data\\Can_t-Hurt-Me-David-Goggins.pdf:121:1\n",
      "------------------------------------------------------------------------------------------\n",
      "What phrase does Goggins use to describe the mental callous formed through suffering?\n",
      "wall of pain and doubt. To push through, you‚Äôll need to channel your\n",
      "darkness, feed off it, and lean on your calloused mind.\n",
      "Remember, visualization will never compensate for work undone. You\n",
      "cannot visualize lies. All the strategies I employ to answer the simple\n",
      "questions and win the mind game are only effective because I put in work.\n",
      "It‚Äôs a lot more than mind over matter. It takes relentless sel\n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Data\\Can_t-Hurt-Me-David-Goggins.pdf:133:2\n",
      "------------------------------------------------------------------------------------------\n",
      "What phrase does Goggins use to describe the mental callous formed through suffering?\n",
      "character influenced me more than I cared to admit. Before that night, I used\n",
      "to tell people that my father had died rather than tell the truth about where I\n",
      "came from. Even in the SEALs I trotted out that lie. I knew why. When you\n",
      "get beat up, you don‚Äôt want to acknowledge getting your ass kicked. It\n",
      "doesn‚Äôt make you feel very manly, so the easiest thing to do is forget about it\n",
      "and move on. Pret\n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Data\\Can_t-Hurt-Me-David-Goggins.pdf:121:0\n",
      "------------------------------------------------------------------------------------------\n",
      "What phrase does Goggins use to describe the mental callous formed through suffering?\n",
      "typhoon.\n",
      "‚ÄúPeople have a hard time going through BUD/S healthy, and you‚Äôre going\n",
      "through it on broken legs! Who else would even think of this?‚Äù I asked.\n",
      "‚ÄúWho else would be able to run even one minute on one broken leg, let alone\n",
      "two? Only Goggins! You are twenty minutes in the business, Goggins! You\n",
      "are a fucking machine! Each step you run from now until the end will only\n",
      "make you harder!‚Äù\n",
      "That las\n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Data\\Can_t-Hurt-Me-David-Goggins.pdf:128:2\n",
      "------------------------------------------------------------------------------------------\n",
      "What phrase does Goggins use to describe the mental callous formed through suffering?\n",
      "there was a schism between the twins after Marcus went through BUD/S.\n",
      "He‚Äôd gained the kind of self-knowledge that can only come from being\n",
      "broken down to nothing and finding more within. Morgan couldn‚Äôt speak\n",
      "that language until he endured it for himself.\n",
      "After surviving two Hell Weeks and participating in three, I was a native\n",
      "speaker. Hell Week was home. It was the fairest place I‚Äôve ever been i\n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Data\\Can_t-Hurt-Me-David-Goggins.pdf:153:2\n",
      "------------------------------------------------------------------------------------------\n",
      "What phrase does Goggins use to describe the mental callous formed through suffering?\n",
      "don‚Äôt know if you could call what I felt on that bed ‚Äúenlightenment,‚Äù but I do\n",
      "know that pain unlocks a secret doorway in the mind. One that leads to both\n",
      "peak performance and beautiful silence.\n",
      "At first, when you push beyond your perceived capability your mind won‚Äôt\n",
      "shut the fuck up about it. It wants you to stop so it sends you into a spin\n",
      "cycle of panic and doubt, which only amplifies your self\n",
      "C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Data\\Can_t-Hurt-Me-David-Goggins.pdf:286:2\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "q = \"What phrase does Goggins use to describe the mental callous formed through suffering?\"\n",
    "results = db.similarity_search_with_score(q, k=8)\n",
    "for doc, score in results:\n",
    "    print(q)\n",
    "    print(doc.page_content[:400])\n",
    "    print(doc.metadata[\"id\"])\n",
    "    print(\"-\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6e3dc-4b59-4e12-8a4a-62f2e8a8bb65",
   "metadata": {},
   "source": [
    "## Loading Json filez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8bf392ad-4bec-4c11-b81f-405e3589abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "faster_path = r\"C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Eval\\faster_rcnn.json\"\n",
    "goggins_path = r\"C:\\Users\\Archit\\Documents\\ML Projects\\RAG-Based-PDF-QA-System\\Eval\\cant_hurt_me.json\"\n",
    "\n",
    "with open(faster_path, \"r\") as f:\n",
    "    faster_data = json.load(f)\n",
    "\n",
    "with open(goggins_path, \"r\") as f:\n",
    "    goggins_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8ce7b4f4-1df3-43f3-a4e2-9b89a11ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Questions: 30\n"
     ]
    }
   ],
   "source": [
    "test_data = faster_data + goggins_data\n",
    "print(\"Total Questions:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc7d044-3fd7-47b0-9628-fc968479986d",
   "metadata": {},
   "source": [
    "## Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ec2b57de-be8d-44e4-b744-a43c46d9d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def normalize_id(raw_id):\n",
    "    # Remove full path\n",
    "    filename_with_pages = os.path.basename(raw_id)\n",
    "\n",
    "    # Split on colon\n",
    "    parts = filename_with_pages.split(\":\")\n",
    "\n",
    "    # Keep filename + page number only\n",
    "    if len(parts) >= 2:\n",
    "        return f\"{parts[0]}:{parts[1]}\"\n",
    "    else:\n",
    "        return filename_with_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d6e25575-f7b9-4b2a-8be6-061eb89958c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Faster-RCNN-Paper.pdf:5'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_id('C:\\\\Users\\\\Archit\\\\Documents\\\\ML Projects\\\\RAG-Based-PDF-QA-System\\\\Data\\\\Faster-RCNN-Paper.pdf:5:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "830d9db2-d1bc-403b-b3b7-a0386cfee4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FAILED:\n",
      "Q: Before losing weight and training, which military unit was Goggins trying to join?\n",
      "Gold: {'Can_t-Hurt-Me-David-Goggins.pdf:73'}\n",
      "Retrieved (normalized): {'Can_t-Hurt-Me-David-Goggins.pdf:137', 'Can_t-Hurt-Me-David-Goggins.pdf:225', 'Can_t-Hurt-Me-David-Goggins.pdf:221', 'Can_t-Hurt-Me-David-Goggins.pdf:227'}\n",
      "\n",
      "FAILED:\n",
      "Q: After serving as a SEAL, which elite Army unit did Goggins consider attempting to join?\n",
      "Gold: {'Can_t-Hurt-Me-David-Goggins.pdf:244'}\n",
      "Retrieved (normalized): {'Can_t-Hurt-Me-David-Goggins.pdf:302', 'Can_t-Hurt-Me-David-Goggins.pdf:141', 'Can_t-Hurt-Me-David-Goggins.pdf:227'}\n",
      "\n",
      "FAILED:\n",
      "Q: What job did Goggins hold before pursuing the Navy SEALs?\n",
      "Gold: {'Can_t-Hurt-Me-David-Goggins.pdf:66'}\n",
      "Retrieved (normalized): {'Can_t-Hurt-Me-David-Goggins.pdf:198', 'Can_t-Hurt-Me-David-Goggins.pdf:302', 'Can_t-Hurt-Me-David-Goggins.pdf:225', 'Can_t-Hurt-Me-David-Goggins.pdf:72'}\n",
      "\n",
      "FAILED:\n",
      "Q: What major ultramarathon did Goggins finish despite severe kidney failure and dehydration?\n",
      "Gold: {'Can_t-Hurt-Me-David-Goggins.pdf:151'}\n",
      "Retrieved (normalized): {'Can_t-Hurt-Me-David-Goggins.pdf:302', 'Can_t-Hurt-Me-David-Goggins.pdf:121', 'Can_t-Hurt-Me-David-Goggins.pdf:189', 'Can_t-Hurt-Me-David-Goggins.pdf:183'}\n",
      "\n",
      "FAILED:\n",
      "Q: What phrase does Goggins use to describe the mental callous formed through suffering?\n",
      "Gold: {'Can_t-Hurt-Me-David-Goggins.pdf:114'}\n",
      "Retrieved (normalized): {'Can_t-Hurt-Me-David-Goggins.pdf:120', 'Can_t-Hurt-Me-David-Goggins.pdf:121', 'Can_t-Hurt-Me-David-Goggins.pdf:111', 'Can_t-Hurt-Me-David-Goggins.pdf:128'}\n",
      "\n",
      "==========\n",
      "Total Questions: 30\n",
      "PASSED 25\n",
      "FAILED 5\n",
      "Recall@4: 0.83\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "total = len(test_data)\n",
    "hits = 0\n",
    "\n",
    "for item in test_data:\n",
    "    question = item[\"question\"]\n",
    "    gold_pages = set(item[\"gold_pages\"])\n",
    "\n",
    "    # Step 1: retrieve more\n",
    "    initial_results = db.similarity_search_with_score(question, k=10)\n",
    "\n",
    "    # Step 2: rerank and take top k\n",
    "    reranked_docs = rerank(question, initial_results, top_n=k)\n",
    "\n",
    "    retrieved_ids = set(\n",
    "        doc.metadata[\"id\"] for doc in reranked_docs\n",
    "    )\n",
    "\n",
    "    normalized_retrieved = set(\n",
    "        normalize_id(r) for r in retrieved_ids\n",
    "    )\n",
    "\n",
    "    if gold_pages & normalized_retrieved:\n",
    "        hits += 1\n",
    "    else:\n",
    "        print(\"\\nFAILED:\")\n",
    "        print(\"Q:\", question)\n",
    "        print(\"Gold:\", gold_pages)\n",
    "        print(\"Retrieved (normalized):\", normalized_retrieved)\n",
    "\n",
    "recall_at_k = hits / total\n",
    "\n",
    "print(\"\\n==========\")\n",
    "print(f\"Total Questions: {total}\")\n",
    "print(f\"PASSED {hits}\")\n",
    "print(f\"FAILED {total - hits}\")\n",
    "print(f\"Recall@{k}: {recall_at_k:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3462cdea-1e7d-4143-9732-c74e9199408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n",
      "Total Questions: 30\n",
      "Recall@4: 0.83\n",
      "Recall@6: 0.87\n",
      "Recall@8: 0.90\n",
      "Recall@10: 0.97\n"
     ]
    }
   ],
   "source": [
    "k_values = [4, 6, 8, 10]\n",
    "total = len(test_data)\n",
    "\n",
    "hits = {k: 0 for k in k_values}\n",
    "\n",
    "for item in test_data:\n",
    "    question = item[\"question\"]\n",
    "    gold_pages = set(item[\"gold_pages\"])\n",
    "\n",
    "    # Step 1: Retrieve once\n",
    "    initial_results = db.similarity_search_with_score(question, k=10)\n",
    "\n",
    "    # Step 2: Rerank once (max needed = 8)\n",
    "    reranked_docs = rerank(question, initial_results, top_n=max(k_values))\n",
    "\n",
    "    # Normalize once\n",
    "    normalized_retrieved = [\n",
    "        normalize_id(doc.metadata[\"id\"]) \n",
    "        for doc in reranked_docs\n",
    "    ]\n",
    "\n",
    "    # Step 3: Check each k\n",
    "    for k in k_values:\n",
    "        top_k = set(normalized_retrieved[:k])\n",
    "        if gold_pages & top_k:\n",
    "            hits[k] += 1\n",
    "\n",
    "print(\"\\n==========\")\n",
    "print(f\"Total Questions: {total}\")\n",
    "\n",
    "for k in k_values:\n",
    "    recall = hits[k] / total\n",
    "    print(f\"Recall@{k}: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733d911f-f20c-423f-96ea-6e05ba232f8d",
   "metadata": {},
   "source": [
    "## Calculating Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "af70c157-3383-4c66-b460-a6b5fed02ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'expected', 'gold_pages'])\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "0776c7b3-6e9b-4267-a24e-08119c5b4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_model = OllamaLLM(model=\"llama3.1\")\n",
    "def judge_answer(question, expected, predicted):\n",
    "    prompt = f\"\"\"\n",
    "You are evaluating a RAG system.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Expected Answer:\n",
    "{expected}\n",
    "\n",
    "Model Answer:\n",
    "{predicted}\n",
    "\n",
    "Is the model answer correct and consistent with the expected answer?\n",
    "\n",
    "Respond with only one word:\n",
    "CORRECT\n",
    "or\n",
    "INCORRECT\n",
    "\"\"\"\n",
    "    result = judge_model.invoke(prompt).strip().upper()\n",
    "    return result == \"CORRECT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37443cc7-38dc-4a92-805c-98f4c63659f6",
   "metadata": {},
   "source": [
    "### For Query Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "1948c8a2-3f30-4747-85c1-4b97d36d01a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_loop():\n",
    "    total = len(test_data)\n",
    "    correct = 0\n",
    "    for item in tqdm(test_data, desc=\"Computing Accuracy\",unit='question'):\n",
    "        \n",
    "        question = item['question']\n",
    "        expected = item['expected']\n",
    "        #predicted = item['expected']\n",
    "        predicted = query_rag(question)\n",
    "    \n",
    "        if judge_answer(question, expected, predicted):\n",
    "            correct += 1\n",
    "        else:\n",
    "            print('\\nWRONG')\n",
    "            print('Question: ', question)\n",
    "            print('Expected: ', expected)\n",
    "            print('Predicted: ', predicted)\n",
    "        accuracy = correct / total\n",
    "\n",
    "    print(\"\\n==========\")\n",
    "    print(f\"Total Questions: {total}\")\n",
    "    print(f\"Correct: {correct}\")\n",
    "    print(f\"Wrong: {total - correct}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "050fcd60-7eb0-4b77-9022-e3b31802bb38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Accuracy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [02:21<00:00,  4.72s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n",
      "Total Questions: 30\n",
      "Correct: 30\n",
      "Wrong: 0\n",
      "Accuracy: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0124ef-3569-43d9-8513-d3fc2cf59b03",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Trial run on 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0503e510-964f-40b4-80be-1f355e422f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The phrase used by Goggins to describe the mental callous formed through suffering is \"calloused mind\".'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = item['question']\n",
    "expected = item['expected']\n",
    "predicted = query_rag(question)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "3a97b687-ece3-4199-9f06-498d290c2454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judge_answer(question, expected, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "0bff9412-e5ca-4eaf-bcbf-41d29448e5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'recruitment staff in San Diego, where the SEALs train.'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rag('What job did Goggins hold before pursuing the Navy SEALs?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "5930687d-76ad-4890-b89d-bc11e5b36643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n",
      "Total Questions: 1\n",
      "Correct: 1\n",
      "Wrong: 0\n",
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# test on qs that got weing ans but the new stricter prompt fixed it\n",
    "total = 1\n",
    "correct = 0\n",
    "question = 'What phrase does Goggins use to describe the mental callous formed through suffering?'\n",
    "expected = 'Callousing the mind.'\n",
    "#predicted = item['expected']\n",
    "predicted = query_rag(question)\n",
    "\n",
    "if judge_answer(question, expected, predicted):\n",
    "    correct += 1\n",
    "else:\n",
    "    print('\\nWRONG')\n",
    "    print('Question: ', question)\n",
    "    print('Expected: ', expected)\n",
    "    print('Predicted: ', predicted)\n",
    "accuracy = correct / total\n",
    "\n",
    "print(\"\\n==========\")\n",
    "print(f\"Total Questions: {total}\")\n",
    "print(f\"Correct: {correct}\")\n",
    "print(f\"Wrong: {total - correct}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701a7e9-dcde-4a8f-a0fd-b57c757af756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c7eb3d-abed-4d17-be24-9ca3ce0c44d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b38483cb-5556-4fa2-89f0-2fe9bedc1082",
   "metadata": {},
   "source": [
    "### On Conversational Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "970afcd2-9991-445a-91b3-8d81af65dfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_loop_chat():\n",
    "    total = len(test_data)\n",
    "    correct = 0\n",
    "\n",
    "    history = []\n",
    "\n",
    "    for item in tqdm(test_data, desc=\"Computing Chat Accuracy\", unit=\"question\"):\n",
    "\n",
    "        question = item[\"question\"]\n",
    "        expected = item[\"expected\"]\n",
    "\n",
    "        predicted = query_rag_hist(question, history)\n",
    "\n",
    "        if judge_answer(question, expected, predicted):\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(\"\\nWRONG\")\n",
    "            print(\"Question:\", question)\n",
    "            print(\"Expected:\", expected)\n",
    "            print(\"Predicted:\", predicted)\n",
    "\n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(\"\\n==========\")\n",
    "    print(f\"Total Questions: {total}\")\n",
    "    print(f\"Correct: {correct}\")\n",
    "    print(f\"Wrong: {total - correct}\")\n",
    "    print(f\"Chat Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "1b057c83-071a-46f4-853c-5ff93fb03384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Chat Accuracy:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 25/30 [05:45<01:15, 15.04s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WRONG\n",
      "Question: What job did Goggins hold before pursuing the Navy SEALs?\n",
      "Expected: He worked as an exterminator.\n",
      "Predicted: According to the provided text, David Goggins held the job of \"car sales\" before becoming a Navy SEAL. However, it's mentioned that he commuted everywhere on a bike and stopped into a Navy recruitment office because he knew he needed structure and purpose, and some warm clothes.\n",
      "\n",
      "Here is an excerpt from the text:\n",
      "\n",
      "\"...He had a good car sales job and no car. He commuted everywhere on a rusted out ten-speed bike, literally freezing his balls off...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Chat Accuracy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [06:44<00:00, 13.48s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n",
      "Total Questions: 30\n",
      "Correct: 29\n",
      "Wrong: 1\n",
      "Chat Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_loop_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3facb7f4-f1ac-4641-bdbd-c7acb241408e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Hallucination Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "b50616c1-97a1-4c9a-8a93-a251135f7b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_hallucination(question, context, answer):\n",
    "    prompt = f\"\"\"\n",
    "You are evaluating whether an answer is grounded in the provided context.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "If the answer is fully supported by the context, respond with YES.\n",
    "If the answer includes unsupported or made-up information, respond with NO.\n",
    "\n",
    "Respond with only YES or NO.\n",
    "\"\"\"\n",
    "\n",
    "    result = model.invoke(prompt).strip().upper()\n",
    "\n",
    "    return result == \"NO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "b227c05e-dd90-4587-a7e7-9fbd58c9cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hallucination_loop(query_function, use_history=False):\n",
    "\n",
    "    total = len(test_data)\n",
    "    hallucinations = 0\n",
    "\n",
    "    history = []\n",
    "\n",
    "    for item in tqdm(test_data, desc=\"Computing Hallucination Rate\", unit=\"question\"):\n",
    "\n",
    "        question = item[\"question\"]\n",
    "\n",
    "        if use_history:\n",
    "            answer, context = query_function(question, history, return_context=True)\n",
    "        else:\n",
    "            answer, context = query_function(question, return_context=True)\n",
    "\n",
    "        if judge_hallucination(question, context, answer):\n",
    "\n",
    "            hallucinations += 1\n",
    "\n",
    "            print(\"\\nHALLUCINATION:\")\n",
    "            print(\"Question:\", question)\n",
    "            print(\"Answer:\", answer)\n",
    "\n",
    "    rate = hallucinations / total\n",
    "\n",
    "    print(\"\\n==========\")\n",
    "    print(f\"Hallucination Rate: {rate:.2f}\")\n",
    "\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "6d239d2d-daba-476e-a280-6aa0e17c218d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Hallucination Rate:  10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                        | 3/30 [00:17<02:40,  5.94s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HALLUCINATION:\n",
      "Question: What task does Faster R-CNN perform?\n",
      "Answer: perform object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Hallucination Rate:  27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                              | 8/30 [00:44<02:06,  5.74s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HALLUCINATION:\n",
      "Question: What is ROI pooling used for?\n",
      "Answer: Adaptively-sized pooling (SPP) [7] on shared conv feature maps is proposed for efÔ¨Åcient region-based object detection [7, 16] and semantic segmentation [2].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Hallucination Rate:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 16/30 [01:30<01:17,  5.54s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HALLUCINATION:\n",
      "Question: How many Hell Weeks did David Goggins complete?\n",
      "Answer: three, I was a native speaker.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Hallucination Rate:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 29/30 [02:26<00:04,  4.08s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HALLUCINATION:\n",
      "Question: What major ultramarathon did Goggins finish despite severe kidney failure and dehydration?\n",
      "Answer: None given in the context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Hallucination Rate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [02:30<00:00,  5.01s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n",
      "Hallucination Rate: 0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13333333333333333"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hallucination_loop(query_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "92bb4eb8-1b7e-45f0-b43b-7e52df02bbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Hallucination Rate:  13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                      | 4/30 [00:43<04:58, 11.49s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HALLUCINATION:\n",
      "Question: What does Faster R-CNN improve over earlier R-CNN variants?\n",
      "Answer: According to the text, Faster R-CNN improves the detection accuracy of earlier R-CNN variants, including SPPnet [7] and Fast R-CNN [5]. Specifically, it achieves state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Hallucination Rate:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 16/30 [03:48<03:12, 13.78s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HALLUCINATION:\n",
      "Question: How many Hell Weeks did David Goggins complete?\n",
      "Answer: There is no information in the provided text about David Goggins or his \"Hell Weeks\". The text appears to be related to a research paper about object detection using Faster R-CNN. \n",
      "\n",
      "If you'd like, I can try to answer one of the original questions based on the context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Hallucination Rate:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 18/30 [04:22<03:04, 15.37s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HALLUCINATION:\n",
      "Question: Before losing weight and training, which military unit was Goggins trying to join?\n",
      "Answer: According to the text, before losing weight and training, David Goggins was trying to join the Naval Academy. However, he wasn't recruited to the Naval Academy.\n",
      "\n",
      "Later in the text, it mentions that after his discharge from the Air Force, he was trying to get into DEVGRU (also known as SEAL Team Six), but it says \"there was a chance\" they would meet there soon, implying that he hadn't yet joined.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Hallucination Rate:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 19/30 [04:38<02:51, 15.57s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HALLUCINATION:\n",
      "Question: After serving as a SEAL, which elite Army unit did Goggins consider attempting to join?\n",
      "Answer: According to the text, before trying to get into the Navy Special Warfare (SEAL) program, David Goggins was trying to join the Naval Academy. However, after being discharged from the Air Force and already serving as a SEAL, there is no indication that he tried to join any elite Army unit.\n",
      "\n",
      "However, it's mentioned in the text that during his first platoon, Goggins put in a request to attend Army Ranger School between deployments, which suggests that he had an interest in attending Ranger School, but this was before serving as a SEAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Hallucination Rate:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 24/30 [05:50<01:33, 15.58s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HALLUCINATION:\n",
      "Question: At what mile during the San Diego One Day race did Goggins loose bowel control\n",
      "Answer: There is no mention in the provided text that David Goggins lost bowel control at any point during the San Diego One Day race. The text does describe a physical training exercise where Goggins vomits blood and has bloody snot streaming from his nose and mouth while carrying a log on his shoulders, but it does not reference this incident happening during a specific mile of a running event.\n",
      "\n",
      "However, the text does mention that in his past, David Goggins had to drop out of one of the ultra-marathon races due to losing control of his bowels. But no information about the San Diego One Day race is provided.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Hallucination Rate:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 26/30 [06:24<01:05, 16.25s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HALLUCINATION:\n",
      "Question: What condition did Goggins suffer from as a child that affected his learning?\n",
      "Answer: Based on the provided text, it is not explicitly stated that David Goggins suffered from a specific condition that affected his learning. However, the text does mention that he had an \"ear infection\" that was so severe that if left untreated, it would have resulted in permanent hearing loss.\n",
      "\n",
      "Additionally, there is a reference to Ritalin, which is often used to treat Attention Deficit Hyperactivity Disorder (ADHD), but the text only mentions that Goggins did not take Ritalin. It does not suggest that he was diagnosed with ADHD or any other learning disability.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Hallucination Rate:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 29/30 [07:07<00:15, 15.08s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HALLUCINATION:\n",
      "Question: What major ultramarathon did Goggins finish despite severe kidney failure and dehydration?\n",
      "Answer: There is no answer to this question as it was not asked. However, I can provide a response.\n",
      "\n",
      "Based on the provided text, it appears that David Goggins finished an ultra-marathon despite suffering from severe symptoms of what doctors believed might be kidney failure, and later diagnosed with an Atrial Septal Defect (ASD). The specific details about the major ultramarathon he finished are not mentioned in the provided context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Hallucination Rate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [07:24<00:00, 14.82s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HALLUCINATION:\n",
      "Question: What phrase does Goggins use to describe the mental callous formed through suffering?\n",
      "Answer: The text doesn't explicitly mention a specific phrase used by Goggins to describe the mental callous formed through suffering. However, it describes his process of re-examining and accepting his past experiences, which ultimately led him to \"find strength in enduring pain and abuse\" and allowed him to \"use where I came from\" as a source of personal growth.\n",
      "\n",
      "It's implied that Goggins developed a mental resilience or toughness through his experiences with suffering, but the text doesn't attribute a specific phrase to this concept.\n",
      "\n",
      "==========\n",
      "Hallucination Rate: 0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26666666666666666"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hallucination_loop(query_rag_hist, use_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b74985-616c-4629-a9f6-2fa30b6aa8dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Calculating Latency for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "3a2ab7c5-521d-42e4-9d56-827aec09e841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latency_loop(query_function, use_history=False):\n",
    "\n",
    "    total = len(test_data)\n",
    "    total_time = 0\n",
    "\n",
    "    history = []  # only used for conversational mode\n",
    "\n",
    "    for item in tqdm(test_data, desc=\"Measuring Latency\", unit=\"question\"):\n",
    "\n",
    "        question = item[\"question\"]\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        if use_history:\n",
    "            answer = query_function(question, history)\n",
    "        else:\n",
    "            answer = query_function(question)\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        total_time += (end - start)\n",
    "\n",
    "    avg_latency = total_time / total\n",
    "\n",
    "    print(\"\\n==========\")\n",
    "    print(f\"Total Questions: {total}\")\n",
    "    print(f\"Average Latency: {avg_latency:.2f} seconds\")\n",
    "\n",
    "    return avg_latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e227d929-dff9-49f1-8895-8fee37380320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring Latency: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [01:53<00:00,  3.78s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n",
      "Total Questions: 30\n",
      "Average Latency: 3.78 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.775906268755595"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latency_loop(query_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "66981cc4-af38-40c4-8a1e-c6b50c12e40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring Latency: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [01:51<00:00,  3.72s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n",
      "Total Questions: 30\n",
      "Average Latency: 3.72 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.7205656369527182"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latency_loop(query_rag, use_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba56a360-0be1-401f-9c37-98187233a115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ceb7c23a-e8c8-4f6f-b324-0cbab311dab4",
   "metadata": {},
   "source": [
    "# UI - Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "38250b08-8061-4c6b-804b-f615ff788beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sources(sources):\n",
    "    formatted = []\n",
    "    for s in sources:\n",
    "        try:\n",
    "            path, page, chunk = s.rsplit(\":\", 2)\n",
    "            filename = os.path.basename(path)\n",
    "            formatted.append(f\"‚Ä¢ {filename} (page {page})\")\n",
    "        except Exception:\n",
    "            formatted.append(f\"‚Ä¢ {s}\")\n",
    "    return \"\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "43436728-bdf9-4a0b-ad97-8a907e63162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_fn(message, history):\n",
    "    response, sources = query_rag_hist(message, CHAT_HISTORY, return_sources=True)\n",
    "    #sources_text = \"\\n\".join([f\"‚Ä¢ {s}\" for s in sources])\n",
    "    sources_text = format_sources(sources)\n",
    "    final_response = f\"\"\"{response}\n",
    "---\n",
    "### Sources\n",
    "{sources_text}\n",
    "\"\"\"\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "850029f3-8699-4e2e-9cce-e24035b2dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_pdf(file):\n",
    "    if file is None:\n",
    "        return 'No file Uploaded'\n",
    "    save_path = Path(DATA_DIR)/file.name\n",
    "    shutil.copy(file.name, save_path)\n",
    "\n",
    "    docs = load_docs()\n",
    "    docs = filter_pages(docs)\n",
    "    chunks = split(docs)\n",
    "    add_to_chroma(chunks)\n",
    "\n",
    "    return f'Indexed: {file.name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "9025c9d0-7f81-4721-b94d-600501d42d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_chat_ui():\n",
    "    CHAT_HISTORY.clear()\n",
    "    return '', \"Chat reset successfully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "3fdacbf3-1a27-477d-b8ff-062fa3f2343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_documents():\n",
    "    files = []\n",
    "\n",
    "    for file in os.listdir(DATA_DIR):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            files.append(file)\n",
    "\n",
    "    if not files:\n",
    "        return \"No documents uploaded.\"\n",
    "\n",
    "    return \"\\n\".join(f\"‚Ä¢ {file}\" for file in files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "d0a4b9f4-0cb8-47f2-ac3f-2221e8d4f46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "gr.ChatInterface(\n",
       "    fn: \u001b[33m'Callable'\u001b[39m,\n",
       "    *,\n",
       "    multimodal: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    chatbot: \u001b[33m'Chatbot | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    textbox: \u001b[33m'Textbox | MultimodalTextbox | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    additional_inputs: \u001b[33m'str | Component | list[str | Component] | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    additional_inputs_accordion: \u001b[33m'str | Accordion | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    additional_outputs: \u001b[33m'Component | list[Component] | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    editable: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    examples: \u001b[33m'list[str] | list[MultimodalValue] | list[list] | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    example_labels: \u001b[33m'list[str] | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    example_icons: \u001b[33m'list[str] | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    run_examples_on_click: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "    cache_examples: \u001b[33m'bool | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    cache_mode: \u001b[33m\"Literal['eager', 'lazy'] | None\"\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    title: \u001b[33m'str | I18nData | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    description: \u001b[33m'str | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    flagging_mode: \u001b[33m\"Literal['never', 'manual'] | None\"\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    flagging_options: \u001b[33m'list[str] | tuple[str, ...] | None'\u001b[39m = (\u001b[33m'Like'\u001b[39m, \u001b[33m'Dislike'\u001b[39m),\n",
       "    flagging_dir: \u001b[33m'str'\u001b[39m = \u001b[33m'.gradio/flagged'\u001b[39m,\n",
       "    analytics_enabled: \u001b[33m'bool | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    autofocus: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "    autoscroll: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "    submit_btn: \u001b[33m'str | bool | None'\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "    stop_btn: \u001b[33m'str | bool | None'\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "    concurrency_limit: \u001b[33m\"int | None | Literal['default']\"\u001b[39m = \u001b[33m'default'\u001b[39m,\n",
       "    delete_cache: \u001b[33m'tuple[int, int] | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    show_progress: \u001b[33m\"Literal['full', 'minimal', 'hidden']\"\u001b[39m = \u001b[33m'minimal'\u001b[39m,\n",
       "    fill_height: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "    fill_width: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    api_name: \u001b[33m'str | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    api_description: \u001b[33m'str | None | Literal[False]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    api_visibility: \u001b[33m\"Literal['public', 'private', 'undocumented']\"\u001b[39m = \u001b[33m'public'\u001b[39m,\n",
       "    save_history: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    validator: \u001b[33m'Callable | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       ")\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "ChatInterface is Gradio's high-level abstraction for creating chatbot UIs, and allows you to create\n",
       "a web-based demo around a chatbot model in a few lines of code. Only one parameter is required: fn, which\n",
       "takes a function that governs the response of the chatbot based on the user input and chat history. Additional\n",
       "parameters can be used to control the appearance and behavior of the demo.\n",
       "\n",
       "Example:\n",
       "    import gradio as gr\n",
       "\n",
       "    def echo(message, history):\n",
       "        return message\n",
       "\n",
       "    demo = gr.ChatInterface(fn=echo, examples=[{\"text\": \"hello\", \"text\": \"hola\", \"text\": \"merhaba\"}], title=\"Echo Bot\")\n",
       "    demo.launch()\n",
       "Demos: chatinterface_random_response, chatinterface_streaming_echo, chatinterface_artifacts\n",
       "Guides: creating-a-chatbot-fast, chatinterface-examples, agents-and-tool-usage, chatbot-specific-events\n",
       "\u001b[31mInit docstring:\u001b[39m\n",
       "Parameters:\n",
       "    fn: the function to wrap the chat interface around. The function should accept two parameters: a `str` representing the input message and `list` of openai-style dictionaries: {\"role\": \"user\" | \"assistant\", \"content\": `str` | {\"path\": `str`} | `gr.Component`} representing the chat history. The function should return/yield a `str` (for a simple message), a supported Gradio component (e.g. gr.Image to return an image), a `dict` (for a complete openai-style message response), or a `list` of such messages.\n",
       "    multimodal: if True, the chat interface will use a `gr.MultimodalTextbox` component for the input, which allows for the uploading of multimedia files. If False, the chat interface will use a gr.Textbox component for the input. If this is True, the first argument of `fn` should accept not a `str` message but a `dict` message with keys \"text\" and \"files\"\n",
       "    chatbot: an instance of the gr.Chatbot component to use for the chat interface, if you would like to customize the chatbot properties. If not provided, a default gr.Chatbot component will be created.\n",
       "    textbox: an instance of the gr.Textbox or gr.MultimodalTextbox component to use for the chat interface, if you would like to customize the textbox properties. If not provided, a default gr.Textbox or gr.MultimodalTextbox component will be created.\n",
       "    editable: if True, users can edit past messages to regenerate responses.\n",
       "    additional_inputs: an instance or list of instances of gradio components (or their string shortcuts) to use as additional inputs to the chatbot. If the components are not already rendered in a surrounding Blocks, then the components will be displayed under the chatbot, in an accordion. The values of these components will be passed into `fn` as arguments in order after the chat history.\n",
       "    additional_inputs_accordion: if a string is provided, this is the label of the `gr.Accordion` to use to contain additional inputs. A `gr.Accordion` object can be provided as well to configure other properties of the container holding the additional inputs. Defaults to a `gr.Accordion(label=\"Additional Inputs\", open=False)`. This parameter is only used if `additional_inputs` is provided.\n",
       "    additional_outputs: an instance or list of instances of gradio components to use as additional outputs from the chat function. These must be components that are already defined in the same Blocks scope. If provided, the chat function should return additional values for these components. See $demo/chatinterface_artifacts.\n",
       "    examples: sample inputs for the function; if provided, appear within the chatbot and can be clicked to populate the chatbot input. Should be a list of strings representing text-only examples, or a list of dictionaries (with keys `text` and `files`) representing multimodal examples. If `additional_inputs` are provided, the examples must be a list of lists, where the first element of each inner list is the string or dictionary example message and the remaining elements are the example values for the additional inputs -- in this case, the examples will appear under the chatbot.\n",
       "    example_labels: labels for the examples, to be displayed instead of the examples themselves. If provided, should be a list of strings with the same length as the examples list. Only applies when examples are displayed within the chatbot (i.e. when `additional_inputs` is not provided).\n",
       "    example_icons: icons for the examples, to be displayed above the examples. If provided, should be a list of string URLs or local paths with the same length as the examples list. Only applies when examples are displayed within the chatbot (i.e. when `additional_inputs` is not provided).\n",
       "    cache_examples: if True, caches examples in the server for fast runtime in examples. The default option in HuggingFace Spaces is True. The default option elsewhere is False.  Note that examples are cached separately from Gradio's queue() so certain features, such as gr.Progress(), gr.Info(), gr.Warning(), etc. will not be displayed in Gradio's UI for cached examples.\n",
       "    cache_mode: if \"eager\", all examples are cached at app launch. If \"lazy\", examples are cached for all users after the first use by any user of the app. If None, will use the GRADIO_CACHE_MODE environment variable if defined, or default to \"eager\".\n",
       "    run_examples_on_click: if True, clicking on an example will run the example through the chatbot fn and the response will be displayed in the chatbot. If False, clicking on an example will only populate the chatbot input with the example message. Has no effect if `cache_examples` is True\n",
       "    title: a title for the interface; if provided, appears above chatbot in large font. Also used as the tab title when opened in a browser window.\n",
       "    description: a description for the interface; if provided, appears above the chatbot and beneath the title in regular font. Accepts Markdown and HTML content.\n",
       "    flagging_mode: one of \"never\", \"manual\". If \"never\", users will not see a button to flag an input and output. If \"manual\", users will see a button to flag.\n",
       "    flagging_options: a list of strings representing the options that users can choose from when flagging a message. Defaults to [\"Like\", \"Dislike\"]. These two case-sensitive strings will render as \"thumbs up\" and \"thumbs down\" icon respectively next to each bot message, but any other strings appear under a separate flag icon.\n",
       "    flagging_dir: path to the the directory where flagged data is stored. If the directory does not exist, it will be created.\n",
       "    analytics_enabled: whether to allow basic telemetry. If None, will use GRADIO_ANALYTICS_ENABLED environment variable if defined, or default to True.\n",
       "    autofocus: if True, autofocuses to the textbox when the page loads.\n",
       "    autoscroll: If True, will automatically scroll to the bottom of the chatbot when a new message appears, unless the user scrolls up. If False, will not scroll to the bottom of the chatbot automatically.\n",
       "    submit_btn: If True, will show a submit button with a submit icon within the textbox. If a string, will use that string as the submit button text in place of the icon. If False, will not show a submit button.\n",
       "    stop_btn: If True, will show a button with a stop icon during generator executions, to stop generating. If a string, will use that string as the submit button text in place of the stop icon. If False, will not show a stop button.\n",
       "    concurrency_limit: if set, this is the maximum number of chatbot submissions that can be running simultaneously. Can be set to None to mean no limit (any number of chatbot submissions can be running simultaneously). Set to \"default\" to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `.queue()`, which is 1 by default).\n",
       "    delete_cache: a tuple corresponding [frequency, age] both expressed in number of seconds. Every `frequency` seconds, the temporary files created by this Blocks instance will be deleted if more than `age` seconds have passed since the file was created. For example, setting this to (86400, 86400) will delete temporary files every day. The cache will be deleted entirely when the server restarts. If None, no cache deletion will occur.\n",
       "    show_progress: how to show the progress animation while event is running: \"full\" shows a spinner which covers the output component area as well as a runtime display in the upper right corner, \"minimal\" only shows the runtime display, \"hidden\" shows no progress animation at all\n",
       "    fill_height: if True, the chat interface will expand to the height of window.\n",
       "    fill_width: Whether to horizontally expand to fill container fully. If False, centers and constrains app to a maximum width.\n",
       "    api_name: defines how the chat endpoint appears in the API docs. Can be a string or None. If set to a string, the endpoint will be exposed in the API docs with the given name. If None, the name of the function will be used.\n",
       "    api_description: Description of the API endpoint. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given description. If None, the function's docstring will be used as the API endpoint description. If False, then no description will be displayed in the API docs.\n",
       "    api_visibility: Controls the visibility of the chat endpoint. Can be \"public\" (shown in API docs and callable), \"private\" (hidden from API docs and not callable), or \"undocumented\" (hidden from API docs but callable).\n",
       "    save_history: if True, will save the chat history to the browser's local storage and display previous conversations in a side panel.\n",
       "    validator: a function that takes in the inputs and can optionally return a gr.validate() object for each input.\n",
       "\u001b[31mFile:\u001b[39m           c:\\users\\archit\\anaconda3\\envs\\dragon\\lib\\site-packages\\gradio\\chat_interface.py\n",
       "\u001b[31mType:\u001b[39m           BlocksMeta\n",
       "\u001b[31mSubclasses:\u001b[39m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gr.ChatInterface?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "6ef8378d-5791-4c06-a920-a7b312f59674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7891\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Local RAG PDF QA System\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale = 3):\n",
    "            chatbot = gr.ChatInterface(fn=chat_fn,\n",
    "                                       title=\"Ask questions about your PDFs\")\n",
    "            \n",
    "            gr.Markdown(\"### Example Questions\")\n",
    "            gr.Examples(examples = [\"What was David Goggins pullup record?\",\n",
    "                                    \"How many Hell Weeks did Goggins complete?\",\n",
    "                                    \"What is Faster R-CNN?\",\n",
    "                                    \"What is ROI pooling used for?\",],\n",
    "                                    inputs = chatbot.textbox)\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"## Documents\")\n",
    "            doc_list = gr.Textbox(value=list_documents(), \n",
    "                                  label=\"Indexed Documents\",\n",
    "                                  interactive=False,\n",
    "                                  lines=4)\n",
    "            gr.Markdown(\"## Upload new PDF\")\n",
    "            file_upload = gr.File(file_types = ['.pdf'],label = 'Select PDF')\n",
    "            upload_btn = gr.Button(\"Add Document\")\n",
    "            upload_status = gr.Textbox(label = 'Upload Status')\n",
    "            \n",
    "            reset_btn = gr.Button(\"Reset Chat\", variant=\"secondary\")\n",
    "            reset_status = gr.Markdown()\n",
    "            \n",
    "    reset_btn.click(fn=reset_chat_ui, outputs=[chatbot.textbox, reset_status])\n",
    "    upload_btn.click(fn=upload_pdf, inputs=file_upload, outputs=upload_status).then(fn=list_documents, outputs=doc_list)\n",
    "\n",
    "demo.launch(inline = False, inbrowser=True)# share=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0660ab-54ab-4f73-b05d-37a240c30150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
